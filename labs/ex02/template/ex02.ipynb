{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height) # x now is already standardized\n",
    "y, tx = build_model_data(x, weight) # tx (2nd col.) now is already standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    mse = np.square((y - tx @ w)).mean()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "        \n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    for i, w0 in enumerate(grid_w0):\n",
    "        for j, w1 in enumerate(grid_w1):\n",
    "            w = np.array([w0, w1])\n",
    "            mse = compute_loss(y, tx, w)\n",
    "            losses[i, j] = mse\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=84.84896629356496, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.011 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "# fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    grad = - 1/ len(y) * (tx.T @ (y - tx @ w))\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-23.293922  ,  -3.47971243])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient(y, tx, np.array([50,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.293922  , -0.47971243])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient(y, tx, np.array([73,13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=530.6049242179212, w0=51.30574540147361, w1=9.435798704492278\n",
      "GD iter. 1/49: loss=75.75675910088253, w0=66.69746902191571, w1=12.266538315840002\n",
      "GD iter. 2/49: loss=34.82042424034894, w0=71.31498610804834, w1=13.11576019924433\n",
      "GD iter. 3/49: loss=31.136154102900914, w0=72.70024123388814, w1=13.37052676426563\n",
      "GD iter. 4/49: loss=30.80456979053059, w0=73.11581777164007, w1=13.446956733772023\n",
      "GD iter. 5/49: loss=30.77472720241726, w0=73.24049073296565, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=30.77204136948706, w0=73.27789262136334, w1=13.476764421879516\n",
      "GD iter. 7/49: loss=30.771799644523348, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=30.77177788927661, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=30.7717759313044, w0=73.29348920882516, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=30.771775755086907, w0=73.29379216412119, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=30.771775739227333, w0=73.29388305071, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=30.771775737799967, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=30.7717757376715, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=30.771775737659947, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=30.7717757376589, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=30.771775737658807, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=30.7717757376588, w0=73.29392197370962, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=30.771775737658807, w0=73.29392199358652, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=30.771775737658796, w0=73.2939219995496, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=30.7717757376588, w0=73.29392200133852, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=30.7717757376588, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=30.7717757376588, w0=73.29392200203618, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=30.7717757376588, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=30.771775737658796, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=30.7717757376588, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=30.7717757376588, w0=73.29392200210464, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=30.7717757376588, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=30.7717757376588, w0=73.29392200210513, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=30.7717757376588, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.024 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7 # 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e689c1653f52463fbdc6b92108ececf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    grad = - 1/ len(y) * (tx.T @ (y - tx @ w))\n",
    "    return grad\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size=batch_size, num_batches=10, shuffle=True): # from from helpers import batch_iter\n",
    "            grad = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=684.8945992184008, w0=47.99306231447285, w1=9.73948215498182\n",
      "SGD iter. 1/49: loss=111.95220800238909, w0=64.53896790434297, w1=11.351048297364983\n",
      "SGD iter. 2/49: loss=39.847258894148446, w0=70.29421387492269, w1=13.201801823092145\n",
      "SGD iter. 3/49: loss=31.833075975405297, w0=72.2665094271654, w1=13.555367167835689\n",
      "SGD iter. 4/49: loss=30.96780167576805, w0=72.87644569898181, w1=13.627155557716732\n",
      "SGD iter. 5/49: loss=30.831910334391008, w0=73.45914546825635, w1=13.660918955195147\n",
      "SGD iter. 6/49: loss=30.88745146373989, w0=73.34650342915033, w1=13.143690233920445\n",
      "SGD iter. 7/49: loss=30.79422918930141, w0=73.20866882903918, w1=13.602941279521437\n",
      "SGD iter. 8/49: loss=30.898598637966717, w0=73.42680434012962, w1=13.810113985453148\n",
      "SGD iter. 9/49: loss=30.85238865409105, w0=73.33677744148257, w1=13.760383644321289\n",
      "SGD iter. 10/49: loss=30.86111669001964, w0=73.47654288514953, w1=13.716335691320603\n",
      "SGD iter. 11/49: loss=30.785842326887185, w0=73.19088098817834, w1=13.538441803088098\n",
      "SGD iter. 12/49: loss=30.809219875794167, w0=73.40887826219561, w1=13.324055130626585\n",
      "SGD iter. 13/49: loss=30.77779035964778, w0=73.2231732338114, w1=13.447943995085696\n",
      "SGD iter. 14/49: loss=31.007460484386264, w0=73.08474170926239, w1=13.04161615354664\n",
      "SGD iter. 15/49: loss=30.852447030019505, w0=73.45573559241369, w1=13.246286527360576\n",
      "SGD iter. 16/49: loss=30.80137825561354, w0=73.15726612196067, w1=13.375176850728616\n",
      "SGD iter. 17/49: loss=30.851593250886467, w0=73.51205632102297, w1=13.300171542326844\n",
      "SGD iter. 18/49: loss=31.310095990061004, w0=73.99701279852137, w1=13.269989798163477\n",
      "SGD iter. 19/49: loss=30.952053589863873, w0=73.66070630285903, w1=13.693598219756508\n",
      "SGD iter. 20/49: loss=30.798342054795498, w0=73.44795058957207, w1=13.533018263747774\n",
      "SGD iter. 21/49: loss=30.80248625929088, w0=73.42332148491364, w1=13.597891517237057\n",
      "SGD iter. 22/49: loss=30.82687069976659, w0=73.48618289924566, w1=13.61436275819154\n",
      "SGD iter. 23/49: loss=30.975621118746687, w0=73.48551680450086, w1=13.888536128997666\n",
      "SGD iter. 24/49: loss=30.91510839442605, w0=73.59802451712274, w1=13.254203417148858\n",
      "SGD iter. 25/49: loss=30.97211800363611, w0=73.73675730727189, w1=13.544821255245825\n",
      "SGD iter. 26/49: loss=30.862568725610547, w0=73.1152348686226, w1=13.722331264183165\n",
      "SGD iter. 27/49: loss=30.95141598859293, w0=73.12896609910007, w1=13.870135027826464\n",
      "SGD iter. 28/49: loss=30.812266361120734, w0=73.2016425986576, w1=13.658528360346084\n",
      "SGD iter. 29/49: loss=30.77520742270254, w0=73.23907605298061, w1=13.500294147948873\n",
      "SGD iter. 30/49: loss=30.79211915562146, w0=73.35372203936568, w1=13.609201328359502\n",
      "SGD iter. 31/49: loss=30.810486715629068, w0=73.42856650704927, w1=13.623176142102135\n",
      "SGD iter. 32/49: loss=30.822011174817675, w0=73.09602199343371, w1=13.374493502630362\n",
      "SGD iter. 33/49: loss=30.790227037030213, w0=73.15868732151178, w1=13.466949969074701\n",
      "SGD iter. 34/49: loss=30.841269100290607, w0=73.35155706713168, w1=13.736950771772655\n",
      "SGD iter. 35/49: loss=30.912909784562373, w0=73.6685877118188, w1=13.452150634097814\n",
      "SGD iter. 36/49: loss=30.776984452719894, w0=73.22615104654497, w1=13.45489686238728\n",
      "SGD iter. 37/49: loss=30.79176612090646, w0=73.18331665232769, w1=13.567785358457375\n",
      "SGD iter. 38/49: loss=30.8380780972539, w0=73.25109553895868, w1=13.73361842871401\n",
      "SGD iter. 39/49: loss=30.779158206590253, w0=73.24020718574518, w1=13.412651362563532\n",
      "SGD iter. 40/49: loss=30.869285875503742, w0=73.49933278373369, w1=13.714907138661786\n",
      "SGD iter. 41/49: loss=30.998529025106073, w0=72.99616754402753, w1=13.851324558233367\n",
      "SGD iter. 42/49: loss=30.775313420121662, w0=73.35083891603793, w1=13.496979379456315\n",
      "SGD iter. 43/49: loss=30.861723423732542, w0=73.0352901043217, w1=13.32786636976951\n",
      "SGD iter. 44/49: loss=30.785440694778245, w0=73.27748975632572, w1=13.595448939248021\n",
      "SGD iter. 45/49: loss=30.831656481050455, w0=73.4583920447182, w1=13.29852096588928\n",
      "SGD iter. 46/49: loss=30.77684651536711, w0=73.31188545030808, w1=13.410806032986235\n",
      "SGD iter. 47/49: loss=30.804241807752266, w0=73.29218065973785, w1=13.299537415021714\n",
      "SGD iter. 48/49: loss=30.832700537462916, w0=73.19003861080003, w1=13.255808346560425\n",
      "SGD iter. 49/49: loss=30.810052740020375, w0=73.20511270334825, w1=13.305385412368134\n",
      "SGD: execution time=0.040 seconds\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 32 # \n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(        # batch_iter(num_batch=1)batchlossbatchsizeloss\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213cbed18ec94c32be114b9b701a18a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=636.56424940319, w0=51.847464098448484, w1=7.724426406192441\n",
      "GD iter. 1/49: loss=177.2847112330252, w0=67.401703327983, w1=10.041754328050121\n",
      "GD iter. 2/49: loss=135.94955279771042, w0=72.06797509684336, w1=10.736952704607413\n",
      "GD iter. 3/49: loss=132.22938853853208, w0=73.46785662750146, w1=10.945512217574594\n",
      "GD iter. 4/49: loss=131.89457375520604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=131.8644404247067, w0=74.01381042445813, w1=11.026850427631796\n",
      "GD iter. 6/49: loss=131.86172842496174, w0=74.05160722578589, w1=11.03248153448191\n",
      "GD iter. 7/49: loss=131.86148434498472, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=131.86146237778675, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=131.86146040073896, w0=74.06736849193958, w1=11.034829706038407\n",
      "GD iter. 10/49: loss=131.86146022280465, w0=74.06767464603033, w1=11.034875318003893\n",
      "GD iter. 11/49: loss=131.86146020679058, w0=74.06776649225756, w1=11.034889001593537\n",
      "GD iter. 12/49: loss=131.86146020534932, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=131.86146020521957, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=131.8614602052079, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=131.86146020520684, w0=74.06780553608876, w1=11.034894818487498\n",
      "GD iter. 16/49: loss=131.86146020520675, w0=74.06780575927509, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=131.86146020520675, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=131.86146020520678, w0=74.06780584631775, w1=11.034894864706558\n",
      "GD iter. 19/49: loss=131.86146020520675, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=131.86146020520675, w0=74.06780585415159, w1=11.034894865873671\n",
      "GD iter. 21/49: loss=131.86146020520673, w0=74.06780585469393, w1=11.03489486595447\n",
      "GD iter. 22/49: loss=131.86146020520673, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=131.86146020520675, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=131.86146020520675, w0=74.0678058549201, w1=11.034894865988164\n",
      "GD iter. 25/49: loss=131.86146020520675, w0=74.06780585492449, w1=11.034894865988818\n",
      "GD iter. 26/49: loss=131.86146020520673, w0=74.06780585492581, w1=11.034894865989017\n",
      "GD iter. 27/49: loss=131.86146020520675, w0=74.06780585492619, w1=11.034894865989077\n",
      "GD iter. 28/49: loss=131.86146020520675, w0=74.06780585492632, w1=11.034894865989093\n",
      "GD iter. 29/49: loss=131.86146020520675, w0=74.06780585492635, w1=11.034894865989099\n",
      "GD iter. 30/49: loss=131.86146020520675, w0=74.06780585492636, w1=11.034894865989102\n",
      "GD iter. 31/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=131.86146020520678, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.004 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "# compare with when no outliers exist (see below): w0=73.61616908174418, w1=14.472539039780616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44360f3cb280414ca7c00fc92ef25191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200,), (200, 2))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=534.100051755883, w0=51.54259072181183, w1=10.132993413506075\n",
      "GD iter. 1/49: loss=72.90005601500053, w0=67.00536793835533, w1=13.172891437557823\n",
      "GD iter. 2/49: loss=31.39205639832127, w0=71.64420110331838, w1=14.084860844773322\n",
      "GD iter. 3/49: loss=27.656336432820154, w0=73.03585105280729, w1=14.358451666937965\n",
      "GD iter. 4/49: loss=27.320121635925045, w0=73.45334603765397, w1=14.440528913587356\n",
      "GD iter. 5/49: loss=27.289862304204483, w0=73.57859453310797, w1=14.46515208758217\n",
      "GD iter. 6/49: loss=27.287138964349634, w0=73.61616908174418, w1=14.472539039780616\n",
      "GD iter. 7/49: loss=27.286893863762707, w0=73.62744144633503, w1=14.474755125440149\n",
      "GD iter. 8/49: loss=27.286871804709882, w0=73.63082315571229, w1=14.47541995113801\n",
      "GD iter. 9/49: loss=27.28686981939512, w0=73.63183766852546, w1=14.475619398847368\n",
      "GD iter. 10/49: loss=27.286869640716795, w0=73.63214202236942, w1=14.475679233160175\n",
      "GD iter. 11/49: loss=27.286869624635752, w0=73.6322333285226, w1=14.475697183454017\n",
      "GD iter. 12/49: loss=27.28686962318846, w0=73.63226072036856, w1=14.47570256854217\n",
      "GD iter. 13/49: loss=27.28686962305819, w0=73.63226893792235, w1=14.475704184068615\n",
      "GD iter. 14/49: loss=27.286869623046467, w0=73.63227140318848, w1=14.475704668726548\n",
      "GD iter. 15/49: loss=27.286869623045412, w0=73.63227214276833, w1=14.47570481412393\n",
      "GD iter. 16/49: loss=27.28686962304532, w0=73.63227236464228, w1=14.475704857743143\n",
      "GD iter. 17/49: loss=27.28686962304531, w0=73.63227243120447, w1=14.475704870828908\n",
      "GD iter. 18/49: loss=27.286869623045312, w0=73.63227245117312, w1=14.475704874754637\n",
      "GD iter. 19/49: loss=27.286869623045305, w0=73.63227245716372, w1=14.475704875932356\n",
      "GD iter. 20/49: loss=27.286869623045312, w0=73.6322724589609, w1=14.475704876285672\n",
      "GD iter. 21/49: loss=27.286869623045312, w0=73.63227245950004, w1=14.475704876391665\n",
      "GD iter. 22/49: loss=27.28686962304531, w0=73.63227245966179, w1=14.475704876423464\n",
      "GD iter. 23/49: loss=27.286869623045312, w0=73.63227245971032, w1=14.475704876433003\n",
      "GD iter. 24/49: loss=27.286869623045312, w0=73.63227245972487, w1=14.475704876435865\n",
      "GD iter. 25/49: loss=27.28686962304531, w0=73.63227245972924, w1=14.475704876436724\n",
      "GD iter. 26/49: loss=27.28686962304531, w0=73.63227245973054, w1=14.475704876436982\n",
      "GD iter. 27/49: loss=27.286869623045312, w0=73.63227245973094, w1=14.47570487643706\n",
      "GD iter. 28/49: loss=27.28686962304531, w0=73.63227245973107, w1=14.475704876437083\n",
      "GD iter. 29/49: loss=27.286869623045312, w0=73.6322724597311, w1=14.475704876437089\n",
      "GD iter. 30/49: loss=27.286869623045312, w0=73.63227245973111, w1=14.47570487643709\n",
      "GD iter. 31/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 32/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 33/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 34/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 35/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 36/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 37/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 38/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 39/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 40/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 41/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 42/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 43/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 44/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 45/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 46/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 47/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 48/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 49/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD: execution time=0.010 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad801193a3054fb9a21c9086db58adcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "\n",
    "    sclar = np.where((y - tx @ w) >= 0, -1, 1) # (N, )\n",
    "    grad = 1/ len(y) * (sclar @ tx).reshape(-1)\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_subgradient_mae(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "        loss = compute_loss(y, tx, w, method=\"MAE\")\n",
    "  \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=73.36780585492637, w0=0.7, w1=8.756471895211877e-16\n",
      "SubGD iter. 1/499: loss=72.66780585492637, w0=1.4, w1=1.7512943790423754e-15\n",
      "SubGD iter. 2/499: loss=71.96780585492637, w0=2.0999999999999996, w1=2.626941568563563e-15\n",
      "SubGD iter. 3/499: loss=71.26780585492638, w0=2.8, w1=3.502588758084751e-15\n",
      "SubGD iter. 4/499: loss=70.56780585492638, w0=3.5, w1=4.378235947605939e-15\n",
      "SubGD iter. 5/499: loss=69.86780585492637, w0=4.2, w1=5.253883137127127e-15\n",
      "SubGD iter. 6/499: loss=69.16780585492639, w0=4.9, w1=6.1295303266483146e-15\n",
      "SubGD iter. 7/499: loss=68.46780585492638, w0=5.6000000000000005, w1=7.0051775161695025e-15\n",
      "SubGD iter. 8/499: loss=67.76780585492638, w0=6.300000000000001, w1=7.88082470569069e-15\n",
      "SubGD iter. 9/499: loss=67.06780585492638, w0=7.000000000000001, w1=8.756471895211878e-15\n",
      "SubGD iter. 10/499: loss=66.36780585492637, w0=7.700000000000001, w1=9.632119084733065e-15\n",
      "SubGD iter. 11/499: loss=65.66780585492639, w0=8.4, w1=1.0507766274254253e-14\n",
      "SubGD iter. 12/499: loss=64.96780585492637, w0=9.1, w1=1.1383413463775441e-14\n",
      "SubGD iter. 13/499: loss=64.26780585492638, w0=9.799999999999999, w1=1.2259060653296629e-14\n",
      "SubGD iter. 14/499: loss=63.567805854926384, w0=10.499999999999998, w1=1.3134707842817817e-14\n",
      "SubGD iter. 15/499: loss=62.86780585492639, w0=11.199999999999998, w1=1.4010355032339005e-14\n",
      "SubGD iter. 16/499: loss=62.167805854926385, w0=11.899999999999997, w1=1.488600222186019e-14\n",
      "SubGD iter. 17/499: loss=61.46780585492638, w0=12.599999999999996, w1=1.576164941138138e-14\n",
      "SubGD iter. 18/499: loss=60.767805854926394, w0=13.299999999999995, w1=1.6637296600902567e-14\n",
      "SubGD iter. 19/499: loss=60.067805854926384, w0=13.999999999999995, w1=1.7512943790423755e-14\n",
      "SubGD iter. 20/499: loss=59.36780585492639, w0=14.699999999999994, w1=1.8388590979944943e-14\n",
      "SubGD iter. 21/499: loss=58.667805854926385, w0=15.399999999999993, w1=1.926423816946613e-14\n",
      "SubGD iter. 22/499: loss=57.96780585492638, w0=16.099999999999994, w1=2.013988535898732e-14\n",
      "SubGD iter. 23/499: loss=57.267805854926394, w0=16.799999999999994, w1=2.1015532548508507e-14\n",
      "SubGD iter. 24/499: loss=56.567805854926384, w0=17.499999999999993, w1=2.1891179738029695e-14\n",
      "SubGD iter. 25/499: loss=55.86780585492639, w0=18.199999999999992, w1=2.2766826927550882e-14\n",
      "SubGD iter. 26/499: loss=55.167805854926385, w0=18.89999999999999, w1=2.364247411707207e-14\n",
      "SubGD iter. 27/499: loss=54.46780585492638, w0=19.59999999999999, w1=2.4518121306593258e-14\n",
      "SubGD iter. 28/499: loss=53.767805854926394, w0=20.29999999999999, w1=2.5393768496114446e-14\n",
      "SubGD iter. 29/499: loss=53.067805854926384, w0=20.99999999999999, w1=2.6269415685635634e-14\n",
      "SubGD iter. 30/499: loss=52.367805854926395, w0=21.69999999999999, w1=2.7145062875156822e-14\n",
      "SubGD iter. 31/499: loss=51.667805854926385, w0=22.399999999999988, w1=2.802071006467801e-14\n",
      "SubGD iter. 32/499: loss=50.96780585492638, w0=23.099999999999987, w1=2.8896357254199195e-14\n",
      "SubGD iter. 33/499: loss=50.267805854926394, w0=23.799999999999986, w1=2.977200444372038e-14\n",
      "SubGD iter. 34/499: loss=49.567805854926405, w0=24.499999999999986, w1=3.064765163324157e-14\n",
      "SubGD iter. 35/499: loss=48.867805854926395, w0=25.199999999999985, w1=3.152329882276276e-14\n",
      "SubGD iter. 36/499: loss=48.1678058549264, w0=25.899999999999984, w1=3.2398946012283946e-14\n",
      "SubGD iter. 37/499: loss=47.4678058549264, w0=26.599999999999984, w1=3.3274593201805134e-14\n",
      "SubGD iter. 38/499: loss=46.7678058549264, w0=27.299999999999983, w1=3.415024039132632e-14\n",
      "SubGD iter. 39/499: loss=46.06780585492639, w0=27.999999999999982, w1=3.502588758084751e-14\n",
      "SubGD iter. 40/499: loss=45.367805854926395, w0=28.69999999999998, w1=3.59015347703687e-14\n",
      "SubGD iter. 41/499: loss=44.6678058549264, w0=29.39999999999998, w1=3.6777181959889886e-14\n",
      "SubGD iter. 42/499: loss=43.9678058549264, w0=30.09999999999998, w1=3.7652829149411074e-14\n",
      "SubGD iter. 43/499: loss=43.2678058549264, w0=30.79999999999998, w1=3.852847633893226e-14\n",
      "SubGD iter. 44/499: loss=42.567805854926405, w0=31.49999999999998, w1=3.940412352845345e-14\n",
      "SubGD iter. 45/499: loss=41.867805854926395, w0=32.19999999999998, w1=4.027977071797464e-14\n",
      "SubGD iter. 46/499: loss=41.1678058549264, w0=32.899999999999984, w1=4.1155417907495825e-14\n",
      "SubGD iter. 47/499: loss=40.4678058549264, w0=33.59999999999999, w1=4.2031065097017013e-14\n",
      "SubGD iter. 48/499: loss=39.767805854926394, w0=34.29999999999999, w1=4.29067122865382e-14\n",
      "SubGD iter. 49/499: loss=39.067805854926384, w0=34.99999999999999, w1=4.378235947605939e-14\n",
      "SubGD iter. 50/499: loss=38.36780585492639, w0=35.699999999999996, w1=4.465800666558058e-14\n",
      "SubGD iter. 51/499: loss=37.66780585492638, w0=36.4, w1=4.5533653855101765e-14\n",
      "SubGD iter. 52/499: loss=36.96780585492638, w0=37.1, w1=4.640930104462295e-14\n",
      "SubGD iter. 53/499: loss=36.26780585492637, w0=37.800000000000004, w1=4.728494823414414e-14\n",
      "SubGD iter. 54/499: loss=35.56780585492637, w0=38.50000000000001, w1=4.816059542366533e-14\n",
      "SubGD iter. 55/499: loss=34.86780585492637, w0=39.20000000000001, w1=4.9036242613186517e-14\n",
      "SubGD iter. 56/499: loss=34.16780585492637, w0=39.90000000000001, w1=4.9911889802707705e-14\n",
      "SubGD iter. 57/499: loss=33.46780585492636, w0=40.600000000000016, w1=5.078753699222889e-14\n",
      "SubGD iter. 58/499: loss=32.767805854926365, w0=41.30000000000002, w1=5.166318418175008e-14\n",
      "SubGD iter. 59/499: loss=32.067805854926355, w0=42.00000000000002, w1=5.253883137127127e-14\n",
      "SubGD iter. 60/499: loss=31.36780585492636, w0=42.700000000000024, w1=5.3414478560792456e-14\n",
      "SubGD iter. 61/499: loss=30.667805854926346, w0=43.40000000000003, w1=5.4290125750313644e-14\n",
      "SubGD iter. 62/499: loss=29.967805854926347, w0=44.10000000000003, w1=5.516577293983483e-14\n",
      "SubGD iter. 63/499: loss=29.267805854926348, w0=44.80000000000003, w1=5.604142012935602e-14\n",
      "SubGD iter. 64/499: loss=28.567805854926338, w0=45.500000000000036, w1=5.691706731887721e-14\n",
      "SubGD iter. 65/499: loss=27.867805854926342, w0=46.20000000000004, w1=5.779271450839839e-14\n",
      "SubGD iter. 66/499: loss=27.173270209668917, w0=46.90000000000004, w1=5.866836169791957e-14\n",
      "SubGD iter. 67/499: loss=26.4904515637512, w0=47.59306930693074, w1=0.01114784567828894\n",
      "SubGD iter. 68/499: loss=25.81721232277017, w0=48.279207920792125, w1=0.03308574108991741\n",
      "SubGD iter. 69/499: loss=25.15503943465645, w0=48.96534653465351, w1=0.055023636501545875\n",
      "SubGD iter. 70/499: loss=24.524103413894778, w0=49.63069306930698, w1=0.1053832638830964\n",
      "SubGD iter. 71/499: loss=23.899295346035586, w0=50.28910891089114, w1=0.16746568532795278\n",
      "SubGD iter. 72/499: loss=23.28439292565714, w0=50.947524752475296, w1=0.22954810677280915\n",
      "SubGD iter. 73/499: loss=22.68687644418184, w0=51.59207920792084, w1=0.312425129327494\n",
      "SubGD iter. 74/499: loss=22.10626756964055, w0=52.22277227722777, w1=0.41195013288401805\n",
      "SubGD iter. 75/499: loss=21.53781882800843, w0=52.84653465346539, w1=0.5208167847923948\n",
      "SubGD iter. 76/499: loss=20.986339874628463, w0=53.4564356435644, w1=0.6457900912636185\n",
      "SubGD iter. 77/499: loss=20.445560936620446, w0=54.0594059405941, w1=0.7796904498577408\n",
      "SubGD iter. 78/499: loss=19.91191015895784, w0=54.655445544554496, w1=0.9197570104995888\n",
      "SubGD iter. 79/499: loss=19.389644090563227, w0=55.24455445544559, w1=1.067092029785011\n",
      "SubGD iter. 80/499: loss=18.887989064395878, w0=55.819801980198065, w1=1.2261255948210965\n",
      "SubGD iter. 81/499: loss=18.41596050185423, w0=56.36732673267331, w1=1.410709342622233\n",
      "SubGD iter. 82/499: loss=17.954898543040382, w0=56.900990099009945, w1=1.605853732220289\n",
      "SubGD iter. 83/499: loss=17.505757656579817, w0=57.42772277227727, w1=1.808762802293982\n",
      "SubGD iter. 84/499: loss=17.074957426931608, w0=57.933663366336674, w1=2.0285064197514897\n",
      "SubGD iter. 85/499: loss=16.652967297509893, w0=58.43267326732677, w1=2.2494370848672975\n",
      "SubGD iter. 86/499: loss=16.248540731496718, w0=58.91089108910895, w1=2.4837982986028537\n",
      "SubGD iter. 87/499: loss=15.849105212654152, w0=59.382178217821824, w1=2.7260245553531703\n",
      "SubGD iter. 88/499: loss=15.466919791231321, w0=59.83960396039608, w1=2.978742333469156\n",
      "SubGD iter. 89/499: loss=15.108294621512211, w0=60.262376237623805, w1=3.251528669355458\n",
      "SubGD iter. 90/499: loss=14.754896345922827, w0=60.67821782178222, w1=3.5270865794243\n",
      "SubGD iter. 91/499: loss=14.404528961620272, w0=61.087128712871326, w1=3.806459183951836\n",
      "SubGD iter. 92/499: loss=14.05578702812727, w0=61.49603960396043, w1=4.085831788479371\n",
      "SubGD iter. 93/499: loss=13.714620911605627, w0=61.891089108910926, w1=4.373839384328629\n",
      "SubGD iter. 94/499: loss=13.381236307284146, w0=62.27920792079211, w1=4.666037469532069\n",
      "SubGD iter. 95/499: loss=13.058821615166227, w0=62.65346534653469, w1=4.959829093241791\n",
      "SubGD iter. 96/499: loss=12.740251724339231, w0=63.02079207920796, w1=5.257057192056662\n",
      "SubGD iter. 97/499: loss=12.423218888756102, w0=63.38118811881192, w1=5.5604343163524295\n",
      "SubGD iter. 98/499: loss=12.107561731901159, w0=63.74158415841588, w1=5.863811440648197\n",
      "SubGD iter. 99/499: loss=11.800622097398126, w0=64.08811881188123, w1=6.172402175278572\n",
      "SubGD iter. 100/499: loss=11.495041794646415, w0=64.42772277227726, w1=6.4863693105165225\n",
      "SubGD iter. 101/499: loss=11.189461491894704, w0=64.7673267326733, w1=6.800336445754473\n",
      "SubGD iter. 102/499: loss=10.883881189142992, w0=65.10693069306933, w1=7.1143035809924235\n",
      "SubGD iter. 103/499: loss=10.58459340831319, w0=65.44653465346536, w1=7.428270716230374\n",
      "SubGD iter. 104/499: loss=10.295816534318933, w0=65.76534653465349, w1=7.747893210218651\n",
      "SubGD iter. 105/499: loss=10.01135208122135, w0=66.070297029703, w1=8.073669686866932\n",
      "SubGD iter. 106/499: loss=9.728084326668117, w0=66.37524752475251, w1=8.399446163515213\n",
      "SubGD iter. 107/499: loss=9.448125461122496, w0=66.6663366336634, w1=8.73297028041742\n",
      "SubGD iter. 108/499: loss=9.171041104096656, w0=66.9574257425743, w1=9.066494397319628\n",
      "SubGD iter. 109/499: loss=8.903656131158947, w0=67.23465346534658, w1=9.398630319470323\n",
      "SubGD iter. 110/499: loss=8.63627115822124, w0=67.51188118811886, w1=9.730766241621017\n",
      "SubGD iter. 111/499: loss=8.376151920302359, w0=67.78910891089114, w1=10.062902163771712\n",
      "SubGD iter. 112/499: loss=8.140540838751482, w0=68.06633663366343, w1=10.363999289979459\n",
      "SubGD iter. 113/499: loss=7.918544501597259, w0=68.32970297029709, w1=10.66046690927365\n",
      "SubGD iter. 114/499: loss=7.7052797283769845, w0=68.59306930693076, w1=10.943174379960851\n",
      "SubGD iter. 115/499: loss=7.493695831178626, w0=68.85643564356442, w1=11.225881850648053\n",
      "SubGD iter. 116/499: loss=7.2899924057434, w0=69.11287128712878, w1=11.504395843582245\n",
      "SubGD iter. 117/499: loss=7.097234035781528, w0=69.35544554455453, w1=11.78820189306779\n",
      "SubGD iter. 118/499: loss=6.919905294668907, w0=69.58415841584166, w1=12.06091146519101\n",
      "SubGD iter. 119/499: loss=6.7505735273154395, w0=69.80594059405948, w1=12.324245668386087\n",
      "SubGD iter. 120/499: loss=6.584744810805652, w0=70.0277227722773, w1=12.587579871581164\n",
      "SubGD iter. 121/499: loss=6.4303432763477915, w0=70.25643564356443, w1=12.824765405096523\n",
      "SubGD iter. 122/499: loss=6.27807148189034, w0=70.47821782178225, w1=13.065616959310187\n",
      "SubGD iter. 123/499: loss=6.133663329263311, w0=70.69306930693077, w1=13.302953389983951\n",
      "SubGD iter. 124/499: loss=6.005840798343018, w0=70.89405940594067, w1=13.525403099312957\n",
      "SubGD iter. 125/499: loss=5.885021825223206, w0=71.08811881188126, w1=13.742945617944251\n",
      "SubGD iter. 126/499: loss=5.771635252269647, w0=71.27524752475254, w1=13.953548196006883\n",
      "SubGD iter. 127/499: loss=5.667162061790248, w0=71.46237623762383, w1=14.164150774069515\n",
      "SubGD iter. 128/499: loss=5.586726765993136, w0=71.62178217821788, w1=14.349779559473214\n",
      "SubGD iter. 129/499: loss=5.523847812160378, w0=71.75346534653471, w1=14.516890107612351\n",
      "SubGD iter. 130/499: loss=5.480093708591866, w0=71.87128712871292, w1=14.670791185324227\n",
      "SubGD iter. 131/499: loss=5.453088003502018, w0=71.95445544554461, w1=14.780276456654562\n",
      "SubGD iter. 132/499: loss=5.4273926308629, w0=72.0376237623763, w1=14.889761727984897\n",
      "SubGD iter. 133/499: loss=5.407322445682747, w0=72.10693069306937, w1=14.985916181776767\n",
      "SubGD iter. 134/499: loss=5.387252260502595, w0=72.17623762376245, w1=15.082070635568638\n",
      "SubGD iter. 135/499: loss=5.370460780338691, w0=72.24554455445552, w1=15.178225089360508\n",
      "SubGD iter. 136/499: loss=5.3574065233347365, w0=72.30099009900998, w1=15.25972348971595\n",
      "SubGD iter. 137/499: loss=5.345929264022579, w0=72.34950495049513, w1=15.335091856448178\n",
      "SubGD iter. 138/499: loss=5.335714659517469, w0=72.39801980198028, w1=15.410460223180406\n",
      "SubGD iter. 139/499: loss=5.330043910465359, w0=72.43267326732682, w1=15.469961786755766\n",
      "SubGD iter. 140/499: loss=5.325676428273224, w0=72.46039603960405, w1=15.51864528583285\n",
      "SubGD iter. 141/499: loss=5.322176726526588, w0=72.48811881188128, w1=15.561592159086528\n",
      "SubGD iter. 142/499: loss=5.32011130964311, w0=72.5019801980199, w1=15.597828332032567\n",
      "SubGD iter. 143/499: loss=5.318478284898438, w0=72.52277227722782, w1=15.624722856626754\n",
      "SubGD iter. 144/499: loss=5.317240048565146, w0=72.55049504950505, w1=15.642690329098041\n",
      "SubGD iter. 145/499: loss=5.316406547951545, w0=72.56435643564366, w1=15.664356578291132\n",
      "SubGD iter. 146/499: loss=5.315557122666141, w0=72.58514851485158, w1=15.677095775361325\n",
      "SubGD iter. 147/499: loss=5.31470769738074, w0=72.6059405940595, w1=15.689834972431518\n",
      "SubGD iter. 148/499: loss=5.313876880922164, w0=72.62673267326743, w1=15.70257416950171\n",
      "SubGD iter. 149/499: loss=5.313052246871382, w0=72.64059405940604, w1=15.724240418694801\n",
      "SubGD iter. 150/499: loss=5.3123778390243865, w0=72.66138613861396, w1=15.736979615764994\n",
      "SubGD iter. 151/499: loss=5.312132229725042, w0=72.66831683168327, w1=15.74811029423132\n",
      "SubGD iter. 152/499: loss=5.311886620425695, w0=72.67524752475258, w1=15.759240972697647\n",
      "SubGD iter. 153/499: loss=5.311683566098434, w0=72.68217821782189, w1=15.770371651163973\n",
      "SubGD iter. 154/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.774323911906727\n",
      "SubGD iter. 155/499: loss=5.311638936484209, w0=72.68217821782189, w1=15.77827617264948\n",
      "SubGD iter. 156/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.782228433392234\n",
      "SubGD iter. 157/499: loss=5.311594306869984, w0=72.68217821782189, w1=15.786180694134988\n",
      "SubGD iter. 158/499: loss=5.311571992062872, w0=72.68217821782189, w1=15.790132954877741\n",
      "SubGD iter. 159/499: loss=5.311549677255758, w0=72.68217821782189, w1=15.794085215620495\n",
      "SubGD iter. 160/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.798037476363248\n",
      "SubGD iter. 161/499: loss=5.311505047641535, w0=72.68217821782189, w1=15.801989737106002\n",
      "SubGD iter. 162/499: loss=5.3114827328344205, w0=72.68217821782189, w1=15.805941997848755\n",
      "SubGD iter. 163/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.809894258591509\n",
      "SubGD iter. 164/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.813846519334263\n",
      "SubGD iter. 165/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.817798780077016\n",
      "SubGD iter. 166/499: loss=5.311393473605971, w0=72.68217821782189, w1=15.82175104081977\n",
      "SubGD iter. 167/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.825703301562523\n",
      "SubGD iter. 168/499: loss=5.311348843991747, w0=72.68217821782189, w1=15.829655562305277\n",
      "SubGD iter. 169/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.83360782304803\n",
      "SubGD iter. 170/499: loss=5.311304214377522, w0=72.68217821782189, w1=15.837560083790784\n",
      "SubGD iter. 171/499: loss=5.31128189957041, w0=72.68217821782189, w1=15.841512344533538\n",
      "SubGD iter. 172/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.845464605276291\n",
      "SubGD iter. 173/499: loss=5.311237269956185, w0=72.68217821782189, w1=15.849416866019045\n",
      "SubGD iter. 174/499: loss=5.311214955149072, w0=72.68217821782189, w1=15.853369126761798\n",
      "SubGD iter. 175/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.857321387504552\n",
      "SubGD iter. 176/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.861273648247305\n",
      "SubGD iter. 177/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.865225908990059\n",
      "SubGD iter. 178/499: loss=5.311125695920622, w0=72.68217821782189, w1=15.869178169732812\n",
      "SubGD iter. 179/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.873130430475566\n",
      "SubGD iter. 180/499: loss=5.311081066306398, w0=72.68217821782189, w1=15.87708269121832\n",
      "SubGD iter. 181/499: loss=5.311058751499286, w0=72.68217821782189, w1=15.881034951961073\n",
      "SubGD iter. 182/499: loss=5.311036436692172, w0=72.68217821782189, w1=15.884987212703827\n",
      "SubGD iter. 183/499: loss=5.31101412188506, w0=72.68217821782189, w1=15.88893947344658\n",
      "SubGD iter. 184/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.892891734189334\n",
      "SubGD iter. 185/499: loss=5.310969492270836, w0=72.68217821782189, w1=15.896843994932087\n",
      "SubGD iter. 186/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.900796255674841\n",
      "SubGD iter. 187/499: loss=5.310924862656612, w0=72.68217821782189, w1=15.904748516417595\n",
      "SubGD iter. 188/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.908700777160348\n",
      "SubGD iter. 189/499: loss=5.310913706061381, w0=72.68217821782189, w1=15.912653037903102\n",
      "SubGD iter. 190/499: loss=5.310892237186269, w0=72.67524752475258, w1=15.910526938117364\n",
      "SubGD iter. 191/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.914479198860118\n",
      "SubGD iter. 192/499: loss=5.310862636053835, w0=72.67524752475258, w1=15.918431459602871\n",
      "SubGD iter. 193/499: loss=5.310859611715928, w0=72.66831683168327, w1=15.916305359817134\n",
      "SubGD iter. 194/499: loss=5.310837296908815, w0=72.66831683168327, w1=15.920257620559887\n",
      "SubGD iter. 195/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.924209881302641\n",
      "SubGD iter. 196/499: loss=5.310823570190172, w0=72.66831683168327, w1=15.928162142045394\n",
      "SubGD iter. 197/499: loss=5.310804671438475, w0=72.66138613861396, w1=15.926036042259657\n",
      "SubGD iter. 198/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.92998830300241\n",
      "SubGD iter. 199/499: loss=5.310772500182623, w0=72.66138613861396, w1=15.933940563745164\n",
      "SubGD iter. 200/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.931814463959427\n",
      "SubGD iter. 201/499: loss=5.310749731161019, w0=72.65445544554466, w1=15.93576672470218\n",
      "SubGD iter. 202/499: loss=5.310727416353907, w0=72.65445544554466, w1=15.939718985444934\n",
      "SubGD iter. 203/499: loss=5.310733434318959, w0=72.65445544554466, w1=15.943671246187687\n",
      "SubGD iter. 204/499: loss=5.310717105690678, w0=72.64752475247535, w1=15.94154514640195\n",
      "SubGD iter. 205/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.945497407144703\n",
      "SubGD iter. 206/499: loss=5.310682364311412, w0=72.64752475247535, w1=15.949449667887457\n",
      "SubGD iter. 207/499: loss=5.310684480220336, w0=72.64059405940604, w1=15.94732356810172\n",
      "SubGD iter. 208/499: loss=5.310662165413224, w0=72.64059405940604, w1=15.951275828844473\n",
      "SubGD iter. 209/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.955228089587226\n",
      "SubGD iter. 210/499: loss=5.310643298447746, w0=72.64059405940604, w1=15.95918035032998\n",
      "SubGD iter. 211/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.957054250544243\n",
      "SubGD iter. 212/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.961006511286996\n",
      "SubGD iter. 213/499: loss=5.3105922284402, w0=72.63366336633673, w1=15.96495877202975\n",
      "SubGD iter. 214/499: loss=5.310633183099026, w0=72.62673267326743, w1=15.962832672244012\n",
      "SubGD iter. 215/499: loss=5.3105993435850625, w0=72.63366336633673, w1=15.967301372051416\n",
      "SubGD iter. 216/499: loss=5.31061822827579, w0=72.62673267326743, w1=15.965175272265679\n",
      "SubGD iter. 217/499: loss=5.310606458729926, w0=72.63366336633673, w1=15.969643972073083\n",
      "SubGD iter. 218/499: loss=5.3106032734525535, w0=72.62673267326743, w1=15.967517872287345\n",
      "SubGD iter. 219/499: loss=5.3106135738747895, w0=72.63366336633673, w1=15.97198657209475\n",
      "SubGD iter. 220/499: loss=5.310588318629316, w0=72.62673267326743, w1=15.969860472309012\n",
      "SubGD iter. 221/499: loss=5.310620689019653, w0=72.63366336633673, w1=15.974329172116416\n",
      "SubGD iter. 222/499: loss=5.310574966149885, w0=72.62673267326743, w1=15.972203072330679\n",
      "SubGD iter. 223/499: loss=5.31058363964973, w0=72.62673267326743, w1=15.970593411609592\n",
      "SubGD iter. 224/499: loss=5.310622915165494, w0=72.63366336633673, w1=15.975062111416996\n",
      "SubGD iter. 225/499: loss=5.310576651555034, w0=72.62673267326743, w1=15.972936011631258\n",
      "SubGD iter. 226/499: loss=5.310578960670142, w0=72.62673267326743, w1=15.971326350910171\n",
      "SubGD iter. 227/499: loss=5.310625141311338, w0=72.63366336633673, w1=15.975795050717576\n",
      "SubGD iter. 228/499: loss=5.31057833696018, w0=72.62673267326743, w1=15.973668950931838\n",
      "SubGD iter. 229/499: loss=5.310574635520698, w0=72.62673267326743, w1=15.972059290210751\n",
      "SubGD iter. 230/499: loss=5.310584557534202, w0=72.62673267326743, w1=15.970449629489664\n",
      "SubGD iter. 231/499: loss=5.31062247845816, w0=72.63366336633673, w1=15.974918329297068\n",
      "SubGD iter. 232/499: loss=5.310576320925845, w0=72.62673267326743, w1=15.972792229511331\n",
      "SubGD iter. 233/499: loss=5.3105798785546146, w0=72.62673267326743, w1=15.971182568790244\n",
      "SubGD iter. 234/499: loss=5.310624704604003, w0=72.63366336633673, w1=15.975651268597648\n",
      "SubGD iter. 235/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.97352516881191\n",
      "SubGD iter. 236/499: loss=5.310575199575027, w0=72.62673267326743, w1=15.971915508090824\n",
      "SubGD iter. 237/499: loss=5.310626930749844, w0=72.63366336633673, w1=15.976384207898228\n",
      "SubGD iter. 238/499: loss=5.310579691736141, w0=72.62673267326743, w1=15.97425810811249\n",
      "SubGD iter. 239/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.972648447391403\n",
      "SubGD iter. 240/499: loss=5.310580796439089, w0=72.62673267326743, w1=15.971038786670317\n",
      "SubGD iter. 241/499: loss=5.310624267896666, w0=72.63366336633673, w1=15.97550748647772\n",
      "SubGD iter. 242/499: loss=5.310577675701806, w0=72.62673267326743, w1=15.973381386691983\n",
      "SubGD iter. 243/499: loss=5.3105761174595, w0=72.62673267326743, w1=15.971771725970896\n",
      "SubGD iter. 244/499: loss=5.310626494042512, w0=72.63366336633673, w1=15.9762404257783\n",
      "SubGD iter. 245/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.974114325992563\n",
      "SubGD iter. 246/499: loss=5.310575659667473, w0=72.62673267326743, w1=15.972504665271476\n",
      "SubGD iter. 247/499: loss=5.310581714323562, w0=72.62673267326743, w1=15.970895004550389\n",
      "SubGD iter. 248/499: loss=5.310623831189333, w0=72.63366336633673, w1=15.975363704357793\n",
      "SubGD iter. 249/499: loss=5.310577345072619, w0=72.62673267326743, w1=15.973237604572056\n",
      "SubGD iter. 250/499: loss=5.310577035343974, w0=72.62673267326743, w1=15.971627943850969\n",
      "SubGD iter. 251/499: loss=5.310626057335176, w0=72.63366336633673, w1=15.976096643658373\n",
      "SubGD iter. 252/499: loss=5.310579030477768, w0=72.62673267326743, w1=15.973970543872635\n",
      "SubGD iter. 253/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.972360883151548\n",
      "SubGD iter. 254/499: loss=5.310582632208036, w0=72.62673267326743, w1=15.970751222430462\n",
      "SubGD iter. 255/499: loss=5.310623394482, w0=72.63366336633673, w1=15.975219922237866\n",
      "SubGD iter. 256/499: loss=5.310577014443433, w0=72.62673267326743, w1=15.973093822452128\n",
      "SubGD iter. 257/499: loss=5.3105779532284485, w0=72.62673267326743, w1=15.971484161731041\n",
      "SubGD iter. 258/499: loss=5.3106256206278415, w0=72.63366336633673, w1=15.975952861538445\n",
      "SubGD iter. 259/499: loss=5.310578699848579, w0=72.62673267326743, w1=15.973826761752708\n",
      "SubGD iter. 260/499: loss=5.310574998409098, w0=72.62673267326743, w1=15.972217101031621\n",
      "SubGD iter. 261/499: loss=5.3105835500925105, w0=72.62673267326743, w1=15.970607440310534\n",
      "SubGD iter. 262/499: loss=5.3106229577746635, w0=72.63366336633673, w1=15.975076140117938\n",
      "SubGD iter. 263/499: loss=5.310576683814246, w0=72.62673267326743, w1=15.9729500403322\n",
      "SubGD iter. 264/499: loss=5.310578871112923, w0=72.62673267326743, w1=15.971340379611114\n",
      "SubGD iter. 265/499: loss=5.310625183920507, w0=72.63366336633673, w1=15.975809079418518\n",
      "SubGD iter. 266/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.97368297963278\n",
      "SubGD iter. 267/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.972073318911693\n",
      "SubGD iter. 268/499: loss=5.310584467976983, w0=72.62673267326743, w1=15.970463658190607\n",
      "SubGD iter. 269/499: loss=5.310622521067329, w0=72.63366336633673, w1=15.97493235799801\n",
      "SubGD iter. 270/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.972806258212273\n",
      "SubGD iter. 271/499: loss=5.310579788997396, w0=72.62673267326743, w1=15.971196597491186\n",
      "SubGD iter. 272/499: loss=5.310624747213171, w0=72.63366336633673, w1=15.97566529729859\n",
      "SubGD iter. 273/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.973539197512853\n",
      "SubGD iter. 274/499: loss=5.310575110017808, w0=72.62673267326743, w1=15.971929536791766\n",
      "SubGD iter. 275/499: loss=5.310626973359014, w0=72.63366336633673, w1=15.97639823659917\n",
      "SubGD iter. 276/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.974272136813433\n",
      "SubGD iter. 277/499: loss=5.310576022555872, w0=72.62673267326743, w1=15.972662476092346\n",
      "SubGD iter. 278/499: loss=5.31058070688187, w0=72.62673267326743, w1=15.971052815371259\n",
      "SubGD iter. 279/499: loss=5.310624310505836, w0=72.63366336633673, w1=15.975521515178663\n",
      "SubGD iter. 280/499: loss=5.310577707961019, w0=72.62673267326743, w1=15.973395415392925\n",
      "SubGD iter. 281/499: loss=5.3105760279022824, w0=72.62673267326743, w1=15.971785754671838\n",
      "SubGD iter. 282/499: loss=5.31062653665168, w0=72.63366336633673, w1=15.976254454479243\n",
      "SubGD iter. 283/499: loss=5.310579393366167, w0=72.62673267326743, w1=15.974128354693505\n",
      "SubGD iter. 284/499: loss=5.310575691926685, w0=72.62673267326743, w1=15.972518693972418\n",
      "SubGD iter. 285/499: loss=5.3105816247663435, w0=72.62673267326743, w1=15.970909033251331\n",
      "SubGD iter. 286/499: loss=5.310623873798502, w0=72.63366336633673, w1=15.975377733058735\n",
      "SubGD iter. 287/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.973251633272998\n",
      "SubGD iter. 288/499: loss=5.310576945786756, w0=72.62673267326743, w1=15.971641972551911\n",
      "SubGD iter. 289/499: loss=5.310626099944345, w0=72.63366336633673, w1=15.976110672359315\n",
      "SubGD iter. 290/499: loss=5.310579062736981, w0=72.62673267326743, w1=15.973984572573578\n",
      "SubGD iter. 291/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.97237491185249\n",
      "SubGD iter. 292/499: loss=5.310582542650818, w0=72.62673267326743, w1=15.970765251131404\n",
      "SubGD iter. 293/499: loss=5.310623437091167, w0=72.63366336633673, w1=15.975233950938808\n",
      "SubGD iter. 294/499: loss=5.310577046702646, w0=72.62673267326743, w1=15.97310785115307\n",
      "SubGD iter. 295/499: loss=5.31057786367123, w0=72.62673267326743, w1=15.971498190431983\n",
      "SubGD iter. 296/499: loss=5.310625663237009, w0=72.63366336633673, w1=15.975966890239388\n",
      "SubGD iter. 297/499: loss=5.310578732107793, w0=72.62673267326743, w1=15.97384079045365\n",
      "SubGD iter. 298/499: loss=5.310575030668311, w0=72.62673267326743, w1=15.972231129732563\n",
      "SubGD iter. 299/499: loss=5.31058346053529, w0=72.62673267326743, w1=15.970621469011476\n",
      "SubGD iter. 300/499: loss=5.310623000383833, w0=72.63366336633673, w1=15.97509016881888\n",
      "SubGD iter. 301/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.972964069033143\n",
      "SubGD iter. 302/499: loss=5.310578781555704, w0=72.62673267326743, w1=15.971354408312056\n",
      "SubGD iter. 303/499: loss=5.310625226529675, w0=72.63366336633673, w1=15.97582310811946\n",
      "SubGD iter. 304/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.973697008333723\n",
      "SubGD iter. 305/499: loss=5.310574700039124, w0=72.62673267326743, w1=15.972087347612636\n",
      "SubGD iter. 306/499: loss=5.310584378419764, w0=72.62673267326743, w1=15.970477686891549\n",
      "SubGD iter. 307/499: loss=5.310622563676497, w0=72.63366336633673, w1=15.974946386698953\n",
      "SubGD iter. 308/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.972820286913215\n",
      "SubGD iter. 309/499: loss=5.310579699440178, w0=72.62673267326743, w1=15.971210626192129\n",
      "SubGD iter. 310/499: loss=5.310624789822341, w0=72.63366336633673, w1=15.975679325999533\n",
      "SubGD iter. 311/499: loss=5.310578070849419, w0=72.62673267326743, w1=15.973553226213795\n",
      "SubGD iter. 312/499: loss=5.310575020460591, w0=72.62673267326743, w1=15.971943565492708\n",
      "SubGD iter. 313/499: loss=5.310627015968183, w0=72.63366336633673, w1=15.976412265300112\n",
      "SubGD iter. 314/499: loss=5.310579756254565, w0=72.62673267326743, w1=15.974286165514375\n",
      "SubGD iter. 315/499: loss=5.310576054815084, w0=72.62673267326743, w1=15.972676504793288\n",
      "SubGD iter. 316/499: loss=5.310580617324651, w0=72.62673267326743, w1=15.971066844072201\n",
      "SubGD iter. 317/499: loss=5.3106243531150055, w0=72.63366336633673, w1=15.975535543879605\n",
      "SubGD iter. 318/499: loss=5.310577740220233, w0=72.62673267326743, w1=15.973409444093868\n",
      "SubGD iter. 319/499: loss=5.310575938345063, w0=72.62673267326743, w1=15.97179978337278\n",
      "SubGD iter. 320/499: loss=5.310626579260847, w0=72.63366336633673, w1=15.976268483180185\n",
      "SubGD iter. 321/499: loss=5.31057942562538, w0=72.62673267326743, w1=15.974142383394447\n",
      "SubGD iter. 322/499: loss=5.310575724185898, w0=72.62673267326743, w1=15.97253272267336\n",
      "SubGD iter. 323/499: loss=5.310581535209124, w0=72.62673267326743, w1=15.970923061952274\n",
      "SubGD iter. 324/499: loss=5.310623916407669, w0=72.63366336633673, w1=15.975391761759678\n",
      "SubGD iter. 325/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.97326566197394\n",
      "SubGD iter. 326/499: loss=5.310576856229536, w0=72.62673267326743, w1=15.971656001252853\n",
      "SubGD iter. 327/499: loss=5.310626142553513, w0=72.63366336633673, w1=15.976124701060257\n",
      "SubGD iter. 328/499: loss=5.310579094996192, w0=72.62673267326743, w1=15.97399860127452\n",
      "SubGD iter. 329/499: loss=5.31057539355671, w0=72.62673267326743, w1=15.972388940553433\n",
      "SubGD iter. 330/499: loss=5.310582453093599, w0=72.62673267326743, w1=15.970779279832346\n",
      "SubGD iter. 331/499: loss=5.310623479700335, w0=72.63366336633673, w1=15.97524797963975\n",
      "SubGD iter. 332/499: loss=5.310577078961858, w0=72.62673267326743, w1=15.973121879854013\n",
      "SubGD iter. 333/499: loss=5.3105777741140106, w0=72.62673267326743, w1=15.971512219132926\n",
      "SubGD iter. 334/499: loss=5.310625705846178, w0=72.63366336633673, w1=15.97598091894033\n",
      "SubGD iter. 335/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.973854819154592\n",
      "SubGD iter. 336/499: loss=5.310575062927524, w0=72.62673267326743, w1=15.972245158433505\n",
      "SubGD iter. 337/499: loss=5.3105833709780725, w0=72.62673267326743, w1=15.970635497712419\n",
      "SubGD iter. 338/499: loss=5.310623042993, w0=72.63366336633673, w1=15.975104197519823\n",
      "SubGD iter. 339/499: loss=5.310576748332672, w0=72.62673267326743, w1=15.972978097734085\n",
      "SubGD iter. 340/499: loss=5.310578691998485, w0=72.62673267326743, w1=15.971368437012998\n",
      "SubGD iter. 341/499: loss=5.310625269138844, w0=72.63366336633673, w1=15.975837136820402\n",
      "SubGD iter. 342/499: loss=5.310578433737819, w0=72.62673267326743, w1=15.973711037034665\n",
      "SubGD iter. 343/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.972101376313578\n",
      "SubGD iter. 344/499: loss=5.310584288862545, w0=72.62673267326743, w1=15.970491715592491\n",
      "SubGD iter. 345/499: loss=5.3106226062856665, w0=72.63366336633673, w1=15.974960415399895\n",
      "SubGD iter. 346/499: loss=5.3105764177034835, w0=72.62673267326743, w1=15.972834315614158\n",
      "SubGD iter. 347/499: loss=5.310579609882959, w0=72.62673267326743, w1=15.97122465489307\n",
      "SubGD iter. 348/499: loss=5.310624832431509, w0=72.63366336633673, w1=15.975693354700475\n",
      "SubGD iter. 349/499: loss=5.3105781031086305, w0=72.62673267326743, w1=15.973567254914737\n",
      "SubGD iter. 350/499: loss=5.310574930903369, w0=72.62673267326743, w1=15.97195759419365\n",
      "SubGD iter. 351/499: loss=5.310627058577351, w0=72.63366336633673, w1=15.976426294001055\n",
      "SubGD iter. 352/499: loss=5.310579788513779, w0=72.62673267326743, w1=15.974300194215317\n",
      "SubGD iter. 353/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.97269053349423\n",
      "SubGD iter. 354/499: loss=5.310580527767431, w0=72.62673267326743, w1=15.971080872773143\n",
      "SubGD iter. 355/499: loss=5.310624395724175, w0=72.63366336633673, w1=15.975549572580547\n",
      "SubGD iter. 356/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.97342347279481\n",
      "SubGD iter. 357/499: loss=5.3105758487878445, w0=72.62673267326743, w1=15.971813812073723\n",
      "SubGD iter. 358/499: loss=5.310626621870016, w0=72.63366336633673, w1=15.976282511881127\n",
      "SubGD iter. 359/499: loss=5.310579457884592, w0=72.62673267326743, w1=15.97415641209539\n",
      "SubGD iter. 360/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.972546751374303\n",
      "SubGD iter. 361/499: loss=5.310581445651906, w0=72.62673267326743, w1=15.970937090653216\n",
      "SubGD iter. 362/499: loss=5.310623959016838, w0=72.63366336633673, w1=15.97540579046062\n",
      "SubGD iter. 363/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.973279690674882\n",
      "SubGD iter. 364/499: loss=5.310576766672318, w0=72.62673267326743, w1=15.971670029953795\n",
      "SubGD iter. 365/499: loss=5.310626185162682, w0=72.63366336633673, w1=15.9761387297612\n",
      "SubGD iter. 366/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.974012629975462\n",
      "SubGD iter. 367/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.972402969254375\n",
      "SubGD iter. 368/499: loss=5.310582363536379, w0=72.62673267326743, w1=15.970793308533288\n",
      "SubGD iter. 369/499: loss=5.310623522309504, w0=72.63366336633673, w1=15.975262008340692\n",
      "SubGD iter. 370/499: loss=5.310577111221071, w0=72.62673267326743, w1=15.973135908554955\n",
      "SubGD iter. 371/499: loss=5.310577684556792, w0=72.62673267326743, w1=15.971526247833868\n",
      "SubGD iter. 372/499: loss=5.310625748455347, w0=72.63366336633673, w1=15.975994947641272\n",
      "SubGD iter. 373/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.973868847855535\n",
      "SubGD iter. 374/499: loss=5.310575095186737, w0=72.62673267326743, w1=15.972259187134448\n",
      "SubGD iter. 375/499: loss=5.310583281420854, w0=72.62673267326743, w1=15.97064952641336\n",
      "SubGD iter. 376/499: loss=5.3106230856021694, w0=72.63366336633673, w1=15.975118226220765\n",
      "SubGD iter. 377/499: loss=5.310576780591883, w0=72.62673267326743, w1=15.972992126435027\n",
      "SubGD iter. 378/499: loss=5.310578602441266, w0=72.62673267326743, w1=15.97138246571394\n",
      "SubGD iter. 379/499: loss=5.310625311748013, w0=72.63366336633673, w1=15.975851165521345\n",
      "SubGD iter. 380/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.973725065735607\n",
      "SubGD iter. 381/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.97211540501452\n",
      "SubGD iter. 382/499: loss=5.310584199305326, w0=72.62673267326743, w1=15.970505744293433\n",
      "SubGD iter. 383/499: loss=5.310622648894835, w0=72.63366336633673, w1=15.974974444100837\n",
      "SubGD iter. 384/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.9728483443151\n",
      "SubGD iter. 385/499: loss=5.310579520325739, w0=72.62673267326743, w1=15.971238683594013\n",
      "SubGD iter. 386/499: loss=5.310624875040677, w0=72.63366336633673, w1=15.975707383401417\n",
      "SubGD iter. 387/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.97358128361568\n",
      "SubGD iter. 388/499: loss=5.310574841346153, w0=72.62673267326743, w1=15.971971622894593\n",
      "SubGD iter. 389/499: loss=5.31062710118652, w0=72.63366336633673, w1=15.976440322701997\n",
      "SubGD iter. 390/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.97431422291626\n",
      "SubGD iter. 391/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.972704562195172\n",
      "SubGD iter. 392/499: loss=5.310580438210212, w0=72.62673267326743, w1=15.971094901474086\n",
      "SubGD iter. 393/499: loss=5.310624438333342, w0=72.63366336633673, w1=15.97556360128149\n",
      "SubGD iter. 394/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.973437501495752\n",
      "SubGD iter. 395/499: loss=5.310575759230625, w0=72.62673267326743, w1=15.971827840774665\n",
      "SubGD iter. 396/499: loss=5.3106266644791855, w0=72.63366336633673, w1=15.97629654058207\n",
      "SubGD iter. 397/499: loss=5.310579490143805, w0=72.62673267326743, w1=15.974170440796332\n",
      "SubGD iter. 398/499: loss=5.310575788704324, w0=72.62673267326743, w1=15.972560780075245\n",
      "SubGD iter. 399/499: loss=5.310581356094687, w0=72.62673267326743, w1=15.970951119354158\n",
      "SubGD iter. 400/499: loss=5.310624001626008, w0=72.63366336633673, w1=15.975419819161562\n",
      "SubGD iter. 401/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.973293719375825\n",
      "SubGD iter. 402/499: loss=5.310576677115099, w0=72.62673267326743, w1=15.971684058654738\n",
      "SubGD iter. 403/499: loss=5.31062622777185, w0=72.63366336633673, w1=15.976152758462142\n",
      "SubGD iter. 404/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.974026658676404\n",
      "SubGD iter. 405/499: loss=5.310575458075137, w0=72.62673267326743, w1=15.972416997955317\n",
      "SubGD iter. 406/499: loss=5.31058227397916, w0=72.62673267326743, w1=15.97080733723423\n",
      "SubGD iter. 407/499: loss=5.310623564918673, w0=72.63366336633673, w1=15.975276037041635\n",
      "SubGD iter. 408/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.973149937255897\n",
      "SubGD iter. 409/499: loss=5.310577594999573, w0=72.62673267326743, w1=15.97154027653481\n",
      "SubGD iter. 410/499: loss=5.310625791064515, w0=72.63366336633673, w1=15.976008976342214\n",
      "SubGD iter. 411/499: loss=5.310578828885431, w0=72.62673267326743, w1=15.973882876556477\n",
      "SubGD iter. 412/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.97227321583539\n",
      "SubGD iter. 413/499: loss=5.310583191863635, w0=72.62673267326743, w1=15.970663555114303\n",
      "SubGD iter. 414/499: loss=5.310623128211339, w0=72.63366336633673, w1=15.975132254921707\n",
      "SubGD iter. 415/499: loss=5.310576812851096, w0=72.62673267326743, w1=15.97300615513597\n",
      "SubGD iter. 416/499: loss=5.310578512884047, w0=72.62673267326743, w1=15.971396494414883\n",
      "SubGD iter. 417/499: loss=5.31062535435718, w0=72.63366336633673, w1=15.975865194222287\n",
      "SubGD iter. 418/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.97373909443655\n",
      "SubGD iter. 419/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.972129433715462\n",
      "SubGD iter. 420/499: loss=5.310584109748109, w0=72.62673267326743, w1=15.970519772994376\n",
      "SubGD iter. 421/499: loss=5.310622691504003, w0=72.63366336633673, w1=15.97498847280178\n",
      "SubGD iter. 422/499: loss=5.31057648222191, w0=72.62673267326743, w1=15.972862373016042\n",
      "SubGD iter. 423/499: loss=5.310579430768521, w0=72.62673267326743, w1=15.971252712294955\n",
      "SubGD iter. 424/499: loss=5.310624917649846, w0=72.63366336633673, w1=15.97572141210236\n",
      "SubGD iter. 425/499: loss=5.310578167627058, w0=72.62673267326743, w1=15.973595312316622\n",
      "SubGD iter. 426/499: loss=5.310574751788933, w0=72.62673267326743, w1=15.971985651595535\n",
      "SubGD iter. 427/499: loss=5.310627143795689, w0=72.63366336633673, w1=15.97645435140294\n",
      "SubGD iter. 428/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.974328251617202\n",
      "SubGD iter. 429/499: loss=5.3105761515927234, w0=72.62673267326743, w1=15.972718590896115\n",
      "SubGD iter. 430/499: loss=5.310580348652993, w0=72.62673267326743, w1=15.971108930175028\n",
      "SubGD iter. 431/499: loss=5.310624480942511, w0=72.63366336633673, w1=15.975577629982432\n",
      "SubGD iter. 432/499: loss=5.310577836997869, w0=72.62673267326743, w1=15.973451530196694\n",
      "SubGD iter. 433/499: loss=5.310575669673406, w0=72.62673267326743, w1=15.971841869475607\n",
      "SubGD iter. 434/499: loss=5.310626707088355, w0=72.63366336633673, w1=15.976310569283012\n",
      "SubGD iter. 435/499: loss=5.310579522403018, w0=72.62673267326743, w1=15.974184469497274\n",
      "SubGD iter. 436/499: loss=5.310575820963535, w0=72.62673267326743, w1=15.972574808776187\n",
      "SubGD iter. 437/499: loss=5.3105812665374685, w0=72.62673267326743, w1=15.9709651480551\n",
      "SubGD iter. 438/499: loss=5.310624044235176, w0=72.63366336633673, w1=15.975433847862504\n",
      "SubGD iter. 439/499: loss=5.310577506368682, w0=72.62673267326743, w1=15.973307748076767\n",
      "SubGD iter. 440/499: loss=5.310576587557881, w0=72.62673267326743, w1=15.97169808735568\n",
      "SubGD iter. 441/499: loss=5.310626270381018, w0=72.63366336633673, w1=15.976166787163084\n",
      "SubGD iter. 442/499: loss=5.310579191773829, w0=72.62673267326743, w1=15.974040687377347\n",
      "SubGD iter. 443/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.97243102665626\n",
      "SubGD iter. 444/499: loss=5.310582184421942, w0=72.62673267326743, w1=15.970821365935173\n",
      "SubGD iter. 445/499: loss=5.310623607527842, w0=72.63366336633673, w1=15.975290065742577\n",
      "SubGD iter. 446/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.97316396595684\n",
      "SubGD iter. 447/499: loss=5.310577505442354, w0=72.62673267326743, w1=15.971554305235752\n",
      "SubGD iter. 448/499: loss=5.310625833673685, w0=72.63366336633673, w1=15.976023005043157\n",
      "SubGD iter. 449/499: loss=5.310578861144642, w0=72.62673267326743, w1=15.97389690525742\n",
      "SubGD iter. 450/499: loss=5.310575159705164, w0=72.62673267326743, w1=15.972287244536332\n",
      "SubGD iter. 451/499: loss=5.310583102306416, w0=72.62673267326743, w1=15.970677583815245\n",
      "SubGD iter. 452/499: loss=5.310623170820506, w0=72.63366336633673, w1=15.97514628362265\n",
      "SubGD iter. 453/499: loss=5.31057684511031, w0=72.62673267326743, w1=15.973020183836912\n",
      "SubGD iter. 454/499: loss=5.310578423326829, w0=72.62673267326743, w1=15.971410523115825\n",
      "SubGD iter. 455/499: loss=5.3106253969663495, w0=72.63366336633673, w1=15.97587922292323\n",
      "SubGD iter. 456/499: loss=5.310578530515457, w0=72.62673267326743, w1=15.973753123137492\n",
      "SubGD iter. 457/499: loss=5.310574829075975, w0=72.62673267326743, w1=15.972143462416405\n",
      "SubGD iter. 458/499: loss=5.31058402019089, w0=72.62673267326743, w1=15.970533801695318\n",
      "SubGD iter. 459/499: loss=5.3106227341131715, w0=72.63366336633673, w1=15.975002501502722\n",
      "SubGD iter. 460/499: loss=5.310576514481121, w0=72.62673267326743, w1=15.972876401716984\n",
      "SubGD iter. 461/499: loss=5.310579341211301, w0=72.62673267326743, w1=15.971266740995897\n",
      "SubGD iter. 462/499: loss=5.310624960259015, w0=72.63366336633673, w1=15.975735440803302\n",
      "SubGD iter. 463/499: loss=5.310578199886269, w0=72.62673267326743, w1=15.973609341017564\n",
      "SubGD iter. 464/499: loss=5.310574662231715, w0=72.62673267326743, w1=15.971999680296477\n",
      "SubGD iter. 465/499: loss=5.310627186404856, w0=72.63366336633673, w1=15.976468380103881\n",
      "SubGD iter. 466/499: loss=5.310579885291418, w0=72.62673267326743, w1=15.974342280318144\n",
      "SubGD iter. 467/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.972732619597057\n",
      "SubGD iter. 468/499: loss=5.310580259095775, w0=72.62673267326743, w1=15.97112295887597\n",
      "SubGD iter. 469/499: loss=5.31062452355168, w0=72.63366336633673, w1=15.975591658683374\n",
      "SubGD iter. 470/499: loss=5.310577869257082, w0=72.62673267326743, w1=15.973465558897637\n",
      "SubGD iter. 471/499: loss=5.310575580116187, w0=72.62673267326743, w1=15.97185589817655\n",
      "SubGD iter. 472/499: loss=5.310626749697523, w0=72.63366336633673, w1=15.976324597983954\n",
      "SubGD iter. 473/499: loss=5.310579554662228, w0=72.62673267326743, w1=15.974198498198216\n",
      "SubGD iter. 474/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.97258883747713\n",
      "SubGD iter. 475/499: loss=5.310581176980249, w0=72.62673267326743, w1=15.970979176756043\n",
      "SubGD iter. 476/499: loss=5.310624086844345, w0=72.63366336633673, w1=15.975447876563447\n",
      "SubGD iter. 477/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.97332177677771\n",
      "SubGD iter. 478/499: loss=5.310576498000661, w0=72.62673267326743, w1=15.971712116056622\n",
      "SubGD iter. 479/499: loss=5.310626312990188, w0=72.63366336633673, w1=15.976180815864026\n",
      "SubGD iter. 480/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.974054716078289\n",
      "SubGD iter. 481/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.972445055357202\n",
      "SubGD iter. 482/499: loss=5.310582094864723, w0=72.62673267326743, w1=15.970835394636115\n",
      "SubGD iter. 483/499: loss=5.31062365013701, w0=72.63366336633673, w1=15.97530409444352\n",
      "SubGD iter. 484/499: loss=5.3105772079987075, w0=72.62673267326743, w1=15.973177994657782\n",
      "SubGD iter. 485/499: loss=5.3105774158851355, w0=72.62673267326743, w1=15.971568333936695\n",
      "SubGD iter. 486/499: loss=5.310625876282853, w0=72.63366336633673, w1=15.976037033744099\n",
      "SubGD iter. 487/499: loss=5.310578893403855, w0=72.62673267326743, w1=15.973910933958361\n",
      "SubGD iter. 488/499: loss=5.310575191964375, w0=72.62673267326743, w1=15.972301273237274\n",
      "SubGD iter. 489/499: loss=5.310583012749197, w0=72.62673267326743, w1=15.970691612516188\n",
      "SubGD iter. 490/499: loss=5.310623213429675, w0=72.63366336633673, w1=15.975160312323592\n",
      "SubGD iter. 491/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.973034212537854\n",
      "SubGD iter. 492/499: loss=5.310578333769609, w0=72.62673267326743, w1=15.971424551816767\n",
      "SubGD iter. 493/499: loss=5.310625439575519, w0=72.63366336633673, w1=15.975893251624171\n",
      "SubGD iter. 494/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.973767151838434\n",
      "SubGD iter. 495/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.972157491117347\n",
      "SubGD iter. 496/499: loss=5.310583930633671, w0=72.62673267326743, w1=15.97054783039626\n",
      "SubGD iter. 497/499: loss=5.310622776722341, w0=72.63366336633673, w1=15.975016530203664\n",
      "SubGD iter. 498/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.972890430417927\n",
      "SubGD iter. 499/499: loss=5.310579251654083, w0=72.62673267326743, w1=15.97128076969684\n",
      "SubGD: execution time=0.027 seconds\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height) # x now is already standardized\n",
    "y, tx = build_model_data(x, weight) # tx (2nd col.) now is already standardized\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "# w0=72.62673267326743, w1=15.97128076969684loss=5.31^2 loss=131.8614MSEMAEoutlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2897c01bf884b0f882461e994af776b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses, subgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size=batch_size, num_batches=10, shuffle=True): # from from helpers import batch_iter\n",
    "            grad = compute_subgradient_mae(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=4754.8529163564, w0=7.000000000000001, w1=-0.1410110908940369\n",
      "SubSGD iter. 1/499: loss=3861.0528801606592, w0=13.999999999999995, w1=0.03261693229171747\n",
      "SubSGD iter. 2/499: loss=3069.241331973683, w0=20.99999999999999, w1=0.02635938154263648\n",
      "SubSGD iter. 3/499: loss=2381.6903862324134, w0=27.999999999999982, w1=-0.26051082570767414\n",
      "SubSGD iter. 4/499: loss=1792.8265664892358, w0=34.99999999999999, w1=-0.569916720840257\n",
      "SubSGD iter. 5/499: loss=1295.963663245691, w0=42.00000000000002, w1=-0.6166299565924865\n",
      "SubSGD iter. 6/499: loss=896.6568080659289, w0=48.912500000000044, w1=-0.4544887208631965\n",
      "SubSGD iter. 7/499: loss=597.5054631209726, w0=55.212500000000034, w1=0.5410183642883976\n",
      "SubSGD iter. 8/499: loss=392.5700023038501, w0=60.112500000000026, w1=2.9134429887628572\n",
      "SubSGD iter. 9/499: loss=259.7918459436635, w0=64.05000000000003, w1=5.783804351490216\n",
      "SubSGD iter. 10/499: loss=181.0237317907785, w0=67.28750000000002, w1=9.248914995099373\n",
      "SubSGD iter. 11/499: loss=152.92072323366614, w0=69.56250000000001, w1=11.9075243303242\n",
      "SubSGD iter. 12/499: loss=149.97140835649148, w0=71.8375, w1=14.659213277051512\n",
      "SubSGD iter. 13/499: loss=158.17209220878325, w0=72.1875, w1=15.807219445845982\n",
      "SubSGD iter. 14/499: loss=156.54115108032897, w0=72.3625, w1=15.700902025893175\n",
      "SubSGD iter. 15/499: loss=159.66548608178167, w0=72.36250000000001, w1=16.024479800402865\n",
      "SubSGD iter. 16/499: loss=159.41742432338404, w0=72.71250000000002, w1=16.10629606859373\n",
      "SubSGD iter. 17/499: loss=155.90472638931683, w0=72.7125, w1=15.747262871961514\n",
      "SubSGD iter. 18/499: loss=157.51521723943694, w0=72.62500000000001, w1=15.890002310651932\n",
      "SubSGD iter. 19/499: loss=157.40364254971507, w0=72.625, w1=15.878498236363118\n",
      "SubSGD iter. 20/499: loss=155.70368119360168, w0=72.44999999999999, w1=15.641946548373499\n",
      "SubSGD iter. 21/499: loss=155.72705283252395, w0=72.62499999999996, w1=15.702217855916764\n",
      "SubSGD iter. 22/499: loss=154.52499162913938, w0=72.62499999999994, w1=15.571616445381283\n",
      "SubSGD iter. 23/499: loss=154.95157747860907, w0=72.44999999999993, w1=15.55958507518222\n",
      "SubSGD iter. 24/499: loss=156.99547437386448, w0=72.62499999999993, w1=15.836178593665245\n",
      "SubSGD iter. 25/499: loss=157.1725497015873, w0=72.62499999999993, w1=15.854583731607974\n",
      "SubSGD iter. 26/499: loss=158.27662546284208, w0=72.53749999999992, w1=15.941352778578711\n",
      "SubSGD iter. 27/499: loss=159.92388442105332, w0=72.71249999999992, w1=16.155985595067104\n",
      "SubSGD iter. 28/499: loss=157.73290910114503, w0=72.18749999999991, w1=15.76098191426356\n",
      "SubSGD iter. 29/499: loss=158.86284913405657, w0=72.71249999999993, w1=16.051321379798704\n",
      "SubSGD iter. 30/499: loss=158.0173444394887, w0=72.53749999999995, w1=15.914858823300664\n",
      "SubSGD iter. 31/499: loss=158.21669657847315, w0=72.53749999999995, w1=15.935241829587126\n",
      "SubSGD iter. 32/499: loss=156.33832960567165, w0=72.44999999999993, w1=15.710317158812568\n",
      "SubSGD iter. 33/499: loss=156.63612229736992, w0=72.62499999999993, w1=15.798609105650542\n",
      "SubSGD iter. 34/499: loss=154.6763624271361, w0=72.88749999999993, w1=15.663261782165446\n",
      "SubSGD iter. 35/499: loss=156.99199852117962, w0=72.71249999999993, w1=15.861247977353315\n",
      "SubSGD iter. 36/499: loss=159.28662208102196, w0=72.53749999999992, w1=16.043220520955344\n",
      "SubSGD iter. 37/499: loss=159.75779306838461, w0=72.62499999999991, w1=16.115706231132254\n",
      "SubSGD iter. 38/499: loss=160.93748032859855, w0=72.36249999999994, w1=16.150356923765063\n",
      "SubSGD iter. 39/499: loss=158.61561738155527, w0=72.27499999999995, w1=15.886699101826537\n",
      "SubSGD iter. 40/499: loss=157.04681624468498, w0=72.53749999999997, w1=15.81438438160227\n",
      "SubSGD iter. 41/499: loss=158.36888701373564, w0=72.53749999999997, w1=15.95074584002268\n",
      "SubSGD iter. 42/499: loss=154.43917305327574, w0=72.62499999999999, w1=15.562148350504755\n",
      "SubSGD iter. 43/499: loss=157.50803311539332, w0=72.36249999999997, w1=15.8033857018134\n",
      "SubSGD iter. 44/499: loss=161.11870198493492, w0=72.09999999999997, w1=16.0732459749379\n",
      "SubSGD iter. 45/499: loss=157.31012140479783, w0=72.53749999999997, w1=15.841850783195861\n",
      "SubSGD iter. 46/499: loss=158.7754932650144, w0=72.88749999999996, w1=16.08671742310986\n",
      "SubSGD iter. 47/499: loss=159.44400602727083, w0=72.71249999999993, w1=16.108916137295147\n",
      "SubSGD iter. 48/499: loss=159.27679494306489, w0=72.62499999999993, w1=16.068148883307135\n",
      "SubSGD iter. 49/499: loss=155.7977284504393, w0=72.53749999999994, w1=15.681875846756128\n",
      "SubSGD iter. 50/499: loss=157.17910959815154, w0=72.88749999999996, w1=15.926165404588579\n",
      "SubSGD iter. 51/499: loss=159.3757292885235, w0=72.62499999999994, w1=16.07796737670187\n",
      "SubSGD iter. 52/499: loss=162.7195127627577, w0=72.27499999999993, w1=16.292641505386293\n",
      "SubSGD iter. 53/499: loss=161.93021274249614, w0=72.97499999999994, w1=16.408397252768875\n",
      "SubSGD iter. 54/499: loss=158.32588989059826, w0=72.79999999999994, w1=16.020584186394633\n",
      "SubSGD iter. 55/499: loss=157.2427005058218, w0=73.14999999999995, w1=15.988567514971656\n",
      "SubSGD iter. 56/499: loss=161.16770007157137, w0=72.71249999999995, w1=16.276019354681086\n",
      "SubSGD iter. 57/499: loss=162.0711749088605, w0=72.71249999999993, w1=16.36151299978243\n",
      "SubSGD iter. 58/499: loss=160.04073376342274, w0=72.79999999999993, w1=16.189692812798487\n",
      "SubSGD iter. 59/499: loss=158.19966894792253, w0=72.36249999999994, w1=15.875363922172546\n",
      "SubSGD iter. 60/499: loss=159.72602585347747, w0=72.71249999999992, w1=16.13663101046706\n",
      "SubSGD iter. 61/499: loss=160.67357275054695, w0=72.8874999999999, w1=16.271209470189895\n",
      "SubSGD iter. 62/499: loss=155.60687681072878, w0=72.6249999999999, w1=15.6893258591946\n",
      "SubSGD iter. 63/499: loss=160.36844387327994, w0=72.62499999999989, w1=16.175448817954113\n",
      "SubSGD iter. 64/499: loss=159.82127385421308, w0=72.53749999999988, w1=16.096315381949424\n",
      "SubSGD iter. 65/499: loss=160.7553678551204, w0=72.44999999999989, w1=16.16096664325474\n",
      "SubSGD iter. 66/499: loss=155.7559458508693, w0=72.71249999999989, w1=15.731450165915268\n",
      "SubSGD iter. 67/499: loss=160.52216131790865, w0=72.4499999999999, w1=16.138168847313988\n",
      "SubSGD iter. 68/499: loss=158.65371672791326, w0=72.88749999999992, w1=16.074650277865043\n",
      "SubSGD iter. 69/499: loss=157.12112329406838, w0=72.79999999999994, w1=15.898261128473958\n",
      "SubSGD iter. 70/499: loss=158.19028899310595, w0=72.97499999999992, w1=16.048337211457703\n",
      "SubSGD iter. 71/499: loss=159.06378079305102, w0=72.71249999999992, w1=16.07130893032559\n",
      "SubSGD iter. 72/499: loss=158.5093922493868, w0=72.36249999999994, w1=15.907251994251942\n",
      "SubSGD iter. 73/499: loss=156.84863657950092, w0=72.44999999999993, w1=15.764575678693054\n",
      "SubSGD iter. 74/499: loss=159.232162247546, w0=72.71249999999993, w1=16.087997683262998\n",
      "SubSGD iter. 75/499: loss=159.69054585499111, w0=72.88749999999995, w1=16.17648642294992\n",
      "SubSGD iter. 76/499: loss=158.2618234027537, w0=72.79999999999994, w1=16.014155002973435\n",
      "SubSGD iter. 77/499: loss=156.3919598655768, w0=72.71249999999995, w1=15.798679674308241\n",
      "SubSGD iter. 78/499: loss=155.19580596634307, w0=72.62499999999993, w1=15.644955280574393\n",
      "SubSGD iter. 79/499: loss=159.57888389085247, w0=72.53749999999992, w1=16.07231361729652\n",
      "SubSGD iter. 80/499: loss=155.44419493161467, w0=72.62499999999991, w1=15.67181690451831\n",
      "SubSGD iter. 81/499: loss=156.13442389165326, w0=72.44999999999992, w1=15.68845994044677\n",
      "SubSGD iter. 82/499: loss=156.64862409283256, w0=72.71249999999992, w1=15.825543040007718\n",
      "SubSGD iter. 83/499: loss=161.5625629628565, w0=72.97499999999992, w1=16.374078148204486\n",
      "SubSGD iter. 84/499: loss=160.28617490075584, w0=72.79999999999991, w1=16.21344514669154\n",
      "SubSGD iter. 85/499: loss=160.31553195674348, w0=72.88749999999993, w1=16.236908883690624\n",
      "SubSGD iter. 86/499: loss=158.92079758740405, w0=72.88749999999995, w1=16.10107838724292\n",
      "SubSGD iter. 87/499: loss=159.9181551440796, w0=72.79999999999994, w1=16.17778930911032\n",
      "SubSGD iter. 88/499: loss=157.55795829061023, w0=73.06249999999991, w1=16.003381378344155\n",
      "SubSGD iter. 89/499: loss=156.03320882482427, w0=73.06249999999991, w1=15.84749382680727\n",
      "SubSGD iter. 90/499: loss=157.0198286548104, w0=73.14999999999989, w1=15.966020585565348\n",
      "SubSGD iter. 91/499: loss=158.52510768363453, w0=72.7999999999999, w1=16.04052327735185\n",
      "SubSGD iter. 92/499: loss=159.0632277252786, w0=72.5374999999999, w1=16.020868342698503\n",
      "SubSGD iter. 93/499: loss=159.5631897560989, w0=72.53749999999992, w1=16.07075562074295\n",
      "SubSGD iter. 94/499: loss=159.35195152647756, w0=72.79999999999991, w1=16.1224443385235\n",
      "SubSGD iter. 95/499: loss=161.3736059851637, w0=72.44999999999993, w1=16.220919354527762\n",
      "SubSGD iter. 96/499: loss=161.54977243221472, w0=72.88749999999996, w1=16.35421722093568\n",
      "SubSGD iter. 97/499: loss=161.58747964161662, w0=72.44999999999996, w1=16.241498716117874\n",
      "SubSGD iter. 98/499: loss=158.90550342996704, w0=72.88749999999997, w1=16.09956872652413\n",
      "SubSGD iter. 99/499: loss=157.84250281093819, w0=72.44999999999996, w1=15.868500851327305\n",
      "SubSGD iter. 100/499: loss=154.361180555874, w0=72.79999999999995, w1=15.605714121317577\n",
      "SubSGD iter. 101/499: loss=154.74891641068857, w0=72.62499999999996, w1=15.596228823338848\n",
      "SubSGD iter. 102/499: loss=158.29929461995985, w0=72.97499999999997, w1=16.05919678547918\n",
      "SubSGD iter. 103/499: loss=159.8569550747324, w0=72.62499999999997, w1=16.125455359148553\n",
      "SubSGD iter. 104/499: loss=160.71734332798775, w0=72.62499999999996, w1=16.20927351349559\n",
      "SubSGD iter. 105/499: loss=159.49193751607677, w0=72.71249999999998, w1=16.113637166054374\n",
      "SubSGD iter. 106/499: loss=161.7897357377389, w0=72.62499999999999, w1=16.311881392172612\n",
      "SubSGD iter. 107/499: loss=159.47627307973323, w0=73.15, w1=16.209104495216486\n",
      "SubSGD iter. 108/499: loss=160.0779951305417, w0=72.62500000000001, w1=16.14712003614008\n",
      "SubSGD iter. 109/499: loss=157.35197076668544, w0=72.62500000000001, w1=15.873161273130089\n",
      "SubSGD iter. 110/499: loss=158.8462259544918, w0=72.45000000000003, w1=15.971236625333887\n",
      "SubSGD iter. 111/499: loss=154.38217356909186, w0=72.62500000000003, w1=15.555848817189602\n",
      "SubSGD iter. 112/499: loss=155.91271187625418, w0=72.62500000000001, w1=15.722064900886243\n",
      "SubSGD iter. 113/499: loss=158.20158620763868, w0=72.62500000000001, w1=15.960180367096847\n",
      "SubSGD iter. 114/499: loss=159.91917615456495, w0=72.8875, w1=16.19867197564292\n",
      "SubSGD iter. 115/499: loss=159.97656662990815, w0=72.8875, w1=16.20422601358721\n",
      "SubSGD iter. 116/499: loss=157.86909270266258, w0=72.97500000000002, w1=16.01620070491272\n",
      "SubSGD iter. 117/499: loss=158.6633645830015, w0=72.88750000000003, w1=16.075607361918756\n",
      "SubSGD iter. 118/499: loss=158.63017485998324, w0=72.53750000000002, w1=15.977250442549195\n",
      "SubSGD iter. 119/499: loss=157.14578573210073, w0=72.97500000000002, w1=15.943061620525125\n",
      "SubSGD iter. 120/499: loss=157.41542225064774, w0=72.53750000000001, w1=15.852791298647144\n",
      "SubSGD iter. 121/499: loss=156.16289274757884, w0=72.62499999999999, w1=15.748677190976359\n",
      "SubSGD iter. 122/499: loss=156.38876767954235, w0=72.79999999999995, w1=15.822376018803169\n",
      "SubSGD iter. 123/499: loss=156.08150502188585, w0=72.36249999999998, w1=15.651384523492183\n",
      "SubSGD iter. 124/499: loss=155.35223153049554, w0=72.71249999999996, w1=15.68827185102814\n",
      "SubSGD iter. 125/499: loss=154.6847162008325, w0=72.71249999999998, w1=15.615986661099523\n",
      "SubSGD iter. 126/499: loss=156.54193541152765, w0=72.44999999999997, w1=15.732040746424028\n",
      "SubSGD iter. 127/499: loss=162.4751189028962, w0=72.36249999999997, w1=16.29850490462639\n",
      "SubSGD iter. 128/499: loss=157.36883052821113, w0=72.36249999999997, w1=15.788767210105856\n",
      "SubSGD iter. 129/499: loss=155.3521111026006, w0=73.06249999999996, w1=15.776203866196166\n",
      "SubSGD iter. 130/499: loss=154.5650093344144, w0=72.88749999999997, w1=15.651216693812\n",
      "SubSGD iter. 131/499: loss=157.10722977306241, w0=72.5375, w1=15.820700290181955\n",
      "SubSGD iter. 132/499: loss=157.15321286823738, w0=72.71249999999999, w1=15.877920645667846\n",
      "SubSGD iter. 133/499: loss=159.6346833878545, w0=72.3625, w1=16.021392146024617\n",
      "SubSGD iter. 134/499: loss=158.47508851466432, w0=72.8875, w1=16.056897096004946\n",
      "SubSGD iter. 135/499: loss=155.9214359126258, w0=72.8875, w1=15.795866804180461\n",
      "SubSGD iter. 136/499: loss=156.91091695622538, w0=72.45, w1=15.771155096042412\n",
      "SubSGD iter. 137/499: loss=156.61693050755588, w0=72.27500000000002, w1=15.67615735252129\n",
      "SubSGD iter. 138/499: loss=155.14998434745493, w0=72.71250000000003, w1=15.666489643368768\n",
      "SubSGD iter. 139/499: loss=158.28461605369554, w0=72.88750000000005, w1=16.037897358224683\n",
      "SubSGD iter. 140/499: loss=155.74331316370353, w0=72.80000000000003, w1=15.754483965975592\n",
      "SubSGD iter. 141/499: loss=156.61001287380503, w0=72.53750000000004, w1=15.768468215898928\n",
      "SubSGD iter. 142/499: loss=158.08034818976026, w0=72.36250000000001, w1=15.863022859912565\n",
      "SubSGD iter. 143/499: loss=155.1599811838559, w0=72.88750000000003, w1=15.715215268213267\n",
      "SubSGD iter. 144/499: loss=156.8553597119968, w0=72.45, w1=15.76528636379941\n",
      "SubSGD iter. 145/499: loss=159.8695743366517, w0=72.18750000000003, w1=15.98187010580734\n",
      "SubSGD iter. 146/499: loss=159.2588104452446, w0=72.62500000000003, w1=16.066361998449384\n",
      "SubSGD iter. 147/499: loss=159.59988298271847, w0=72.625, w1=16.100142544286967\n",
      "SubSGD iter. 148/499: loss=154.04168653591594, w0=72.62499999999999, w1=15.518034122771045\n",
      "SubSGD iter. 149/499: loss=157.64838208201053, w0=72.45, w1=15.848378645183727\n",
      "SubSGD iter. 150/499: loss=158.88156483885882, w0=72.8, w1=16.07600317149757\n",
      "SubSGD iter. 151/499: loss=159.78691605716853, w0=72.53750000000001, w1=16.092920156785887\n",
      "SubSGD iter. 152/499: loss=160.65118711295636, w0=72.45000000000002, w1=16.15079470116137\n",
      "SubSGD iter. 153/499: loss=159.70863897861543, w0=72.88750000000003, w1=16.178245608669915\n",
      "SubSGD iter. 154/499: loss=158.81481293069908, w0=72.45000000000002, w1=15.96805378713267\n",
      "SubSGD iter. 155/499: loss=158.43247384592289, w0=72.53750000000004, w1=15.95720912161057\n",
      "SubSGD iter. 156/499: loss=163.07188848590903, w0=72.10000000000005, w1=16.26348630135867\n",
      "SubSGD iter. 157/499: loss=157.17483148848555, w0=72.71250000000006, w1=15.880152064826055\n",
      "SubSGD iter. 158/499: loss=157.90609676549855, w0=72.62500000000007, w1=15.930091269125773\n",
      "SubSGD iter. 159/499: loss=158.10061158821992, w0=72.97500000000007, w1=16.03938552403333\n",
      "SubSGD iter. 160/499: loss=156.69262548899744, w0=72.88750000000005, w1=15.876180167705717\n",
      "SubSGD iter. 161/499: loss=157.01467399948237, w0=72.80000000000004, w1=15.887304793900196\n",
      "SubSGD iter. 162/499: loss=159.03145543078605, w0=72.80000000000005, w1=16.090848143047136\n",
      "SubSGD iter. 163/499: loss=157.0453436381028, w0=72.71250000000008, w1=15.866771258499424\n",
      "SubSGD iter. 164/499: loss=158.51593528851248, w0=72.53750000000007, w1=15.965679699429316\n",
      "SubSGD iter. 165/499: loss=157.63764919392844, w0=72.97500000000008, w1=15.992915070885457\n",
      "SubSGD iter. 166/499: loss=157.72081731068752, w0=72.5375000000001, w1=15.884381549738132\n",
      "SubSGD iter. 167/499: loss=158.2382832583532, w0=72.97500000000008, w1=16.05312148667746\n",
      "SubSGD iter. 168/499: loss=156.57571413343308, w0=72.71250000000009, w1=15.81792737349194\n",
      "SubSGD iter. 169/499: loss=158.09281827045075, w0=72.71250000000009, w1=15.973974143025114\n",
      "SubSGD iter. 170/499: loss=158.2605840851501, w0=72.71250000000009, w1=15.990928554287228\n",
      "SubSGD iter. 171/499: loss=155.7036150173117, w0=72.88750000000009, w1=15.772935906433105\n",
      "SubSGD iter. 172/499: loss=159.50045096283932, w0=72.8000000000001, w1=16.137017862531547\n",
      "SubSGD iter. 173/499: loss=159.07722290503102, w0=72.5375000000001, w1=16.02227160032577\n",
      "SubSGD iter. 174/499: loss=159.67885036470645, w0=72.36250000000008, w1=16.02581883862722\n",
      "SubSGD iter. 175/499: loss=162.20210677914295, w0=72.36250000000008, w1=16.272506775542\n",
      "SubSGD iter. 176/499: loss=163.61983845733855, w0=72.62500000000009, w1=16.482526417142523\n",
      "SubSGD iter. 177/499: loss=158.2078526055625, w0=72.45000000000007, w1=15.906146924353763\n",
      "SubSGD iter. 178/499: loss=156.3518407943703, w0=72.45000000000006, w1=15.711761852009733\n",
      "SubSGD iter. 179/499: loss=159.66613536423404, w0=72.71250000000006, w1=16.130758011583747\n",
      "SubSGD iter. 180/499: loss=158.74517299360204, w0=72.36250000000007, w1=15.931387971217513\n",
      "SubSGD iter. 181/499: loss=159.45587619699924, w0=72.71250000000006, w1=16.110085702932572\n",
      "SubSGD iter. 182/499: loss=158.9135133577172, w0=72.88750000000007, w1=16.100359429218933\n",
      "SubSGD iter. 183/499: loss=159.9457211346652, w0=72.88750000000007, w1=16.201241642802014\n",
      "SubSGD iter. 184/499: loss=158.42574540965367, w0=72.97500000000008, w1=16.07176498223051\n",
      "SubSGD iter. 185/499: loss=158.64571537603666, w0=73.15000000000008, w1=16.128212800645542\n",
      "SubSGD iter. 186/499: loss=155.3328728830092, w0=73.15000000000006, w1=15.791894455064705\n",
      "SubSGD iter. 187/499: loss=155.93897706547176, w0=72.97500000000007, w1=15.818543287821944\n",
      "SubSGD iter. 188/499: loss=156.9327775516013, w0=72.71250000000005, w1=15.85510890542729\n",
      "SubSGD iter. 189/499: loss=160.2344704902187, w0=72.62500000000004, w1=16.16240123361092\n",
      "SubSGD iter. 190/499: loss=159.03657949542398, w0=72.88750000000005, w1=16.112492465153874\n",
      "SubSGD iter. 191/499: loss=157.5886646753964, w0=72.80000000000004, w1=15.946093577546613\n",
      "SubSGD iter. 192/499: loss=159.4282577158922, w0=72.53750000000004, w1=16.057340634841527\n",
      "SubSGD iter. 193/499: loss=159.82423450086878, w0=72.53750000000005, w1=16.09660784541023\n",
      "SubSGD iter. 194/499: loss=159.22389670267177, w0=72.53750000000004, w1=16.036954490568082\n",
      "SubSGD iter. 195/499: loss=158.28162480217802, w0=73.06250000000006, w1=16.075680988710648\n",
      "SubSGD iter. 196/499: loss=157.45703605030187, w0=72.71250000000005, w1=15.909186802742333\n",
      "SubSGD iter. 197/499: loss=158.50838215562976, w0=72.80000000000005, w1=16.038852326303648\n",
      "SubSGD iter. 198/499: loss=159.66897248200203, w0=72.27500000000006, w1=15.994064095137997\n",
      "SubSGD iter. 199/499: loss=161.0990599926874, w0=72.36250000000007, w1=16.16612588092448\n",
      "SubSGD iter. 200/499: loss=160.77727374745436, w0=72.71250000000006, w1=16.2386396312375\n",
      "SubSGD iter. 201/499: loss=158.38125968833822, w0=72.80000000000007, w1=16.026133970394547\n",
      "SubSGD iter. 202/499: loss=160.20653061548833, w0=72.88750000000009, w1=16.22642147182561\n",
      "SubSGD iter. 203/499: loss=159.18037984202874, w0=72.62500000000009, w1=16.05856194311637\n",
      "SubSGD iter. 204/499: loss=160.54408918742547, w0=72.88750000000007, w1=16.258830841003096\n",
      "SubSGD iter. 205/499: loss=159.66280137761407, w0=72.80000000000004, w1=16.152903217547536\n",
      "SubSGD iter. 206/499: loss=158.6781395114861, w0=72.53750000000002, w1=15.982100470838924\n",
      "SubSGD iter. 207/499: loss=157.8731216831966, w0=72.80000000000001, w1=15.974968731044594\n",
      "SubSGD iter. 208/499: loss=158.26833573773138, w0=72.53750000000002, w1=15.940507928889884\n",
      "SubSGD iter. 209/499: loss=158.14450763534632, w0=72.80000000000003, w1=16.002360592529556\n",
      "SubSGD iter. 210/499: loss=155.19988186666546, w0=72.71250000000002, w1=15.671873160208419\n",
      "SubSGD iter. 211/499: loss=157.51032161192933, w0=72.53750000000001, w1=15.862629883272124\n",
      "SubSGD iter. 212/499: loss=154.05950253032958, w0=72.45000000000003, w1=15.459908599401565\n",
      "SubSGD iter. 213/499: loss=154.54103101409265, w0=72.71250000000002, w1=15.600277308730158\n",
      "SubSGD iter. 214/499: loss=157.8974164540676, w0=72.53750000000001, w1=15.902555519654163\n",
      "SubSGD iter. 215/499: loss=159.64285813604081, w0=72.62500000000001, w1=16.104382926515953\n",
      "SubSGD iter. 216/499: loss=159.36055833945022, w0=72.7125, w1=16.10068643037919\n",
      "SubSGD iter. 217/499: loss=155.7740109513307, w0=72.8875, w1=15.78035889334728\n",
      "SubSGD iter. 218/499: loss=155.56817153733817, w0=72.53750000000002, w1=15.657110278808972\n",
      "SubSGD iter. 219/499: loss=154.91821654377307, w0=72.53750000000002, w1=15.58625953265167\n",
      "SubSGD iter. 220/499: loss=160.52857484026242, w0=72.36250000000004, w1=16.110231955898467\n",
      "SubSGD iter. 221/499: loss=159.67232729949453, w0=72.71250000000005, w1=16.13136552065692\n",
      "SubSGD iter. 222/499: loss=160.34453653606386, w0=72.53750000000005, w1=16.147745374388048\n",
      "SubSGD iter. 223/499: loss=158.61161274336328, w0=72.18750000000006, w1=15.853048291326074\n",
      "SubSGD iter. 224/499: loss=160.40744595907609, w0=72.53750000000007, w1=16.15389376657969\n",
      "SubSGD iter. 225/499: loss=158.83796011021914, w0=72.97500000000005, w1=16.112519829340684\n",
      "SubSGD iter. 226/499: loss=155.66140506787707, w0=72.80000000000007, w1=15.745798513579969\n",
      "SubSGD iter. 227/499: loss=159.27454495456968, w0=72.8000000000001, w1=16.11483119074531\n",
      "SubSGD iter. 228/499: loss=157.420249719122, w0=72.88750000000012, w1=15.950753651871997\n",
      "SubSGD iter. 229/499: loss=155.27746085198004, w0=72.80000000000011, w1=15.704870129412669\n",
      "SubSGD iter. 230/499: loss=157.9567178480731, w0=72.71250000000013, w1=15.960176957653399\n",
      "SubSGD iter. 231/499: loss=157.0043726170217, w0=72.80000000000014, w1=15.88624320702067\n",
      "SubSGD iter. 232/499: loss=162.681719991712, w0=72.18750000000013, w1=16.258371646681763\n",
      "SubSGD iter. 233/499: loss=162.88037642723697, w0=72.53750000000012, w1=16.389993390984575\n",
      "SubSGD iter. 234/499: loss=157.00466581085814, w0=72.36250000000013, w1=15.750309751956788\n",
      "SubSGD iter. 235/499: loss=154.26343600365857, w0=73.06250000000014, w1=15.659971723350811\n",
      "SubSGD iter. 236/499: loss=155.10666034453854, w0=73.06250000000014, w1=15.75024854067596\n",
      "SubSGD iter. 237/499: loss=156.93363742662453, w0=72.62500000000014, w1=15.829734642916836\n",
      "SubSGD iter. 238/499: loss=157.4273655949227, w0=72.62500000000014, w1=15.880946522204352\n",
      "SubSGD iter. 239/499: loss=158.05992511261115, w0=72.80000000000014, w1=15.993839634951467\n",
      "SubSGD iter. 240/499: loss=160.6368699058988, w0=72.36250000000015, w1=16.120889522086024\n",
      "SubSGD iter. 241/499: loss=160.7963033296907, w0=72.62500000000018, w1=16.216897798203117\n",
      "SubSGD iter. 242/499: loss=160.41236059144052, w0=72.62500000000017, w1=16.179718638600217\n",
      "SubSGD iter. 243/499: loss=161.65031974564022, w0=72.71250000000018, w1=16.32186049667333\n",
      "SubSGD iter. 244/499: loss=158.8679266744749, w0=72.88750000000017, w1=16.095857675385655\n",
      "SubSGD iter. 245/499: loss=156.1534544362255, w0=73.32500000000017, w1=15.907289933397546\n",
      "SubSGD iter. 246/499: loss=159.55797213949293, w0=72.88750000000016, w1=16.163577935073516\n",
      "SubSGD iter. 247/499: loss=160.36883565815853, w0=72.36250000000017, w1=16.094470677666028\n",
      "SubSGD iter. 248/499: loss=156.78705605328992, w0=72.71250000000016, w1=15.839969463504309\n",
      "SubSGD iter. 249/499: loss=157.1176479574845, w0=72.80000000000017, w1=15.897903817913065\n",
      "SubSGD iter. 250/499: loss=160.7422030103739, w0=72.62500000000016, w1=16.211675146255768\n",
      "SubSGD iter. 251/499: loss=160.49937728503247, w0=72.45000000000017, w1=16.135936063195047\n",
      "SubSGD iter. 252/499: loss=155.72846742712977, w0=72.53750000000018, w1=15.674417601389981\n",
      "SubSGD iter. 253/499: loss=160.52411211312707, w0=72.53750000000018, w1=16.165276518293243\n",
      "SubSGD iter. 254/499: loss=159.67413409001708, w0=72.80000000000017, w1=16.154010238690248\n",
      "SubSGD iter. 255/499: loss=158.1743988946777, w0=72.80000000000018, w1=16.00536838505002\n",
      "SubSGD iter. 256/499: loss=155.6710240314547, w0=72.45000000000017, w1=15.638400925723845\n",
      "SubSGD iter. 257/499: loss=158.64626154583087, w0=72.53750000000018, w1=15.978877605744854\n",
      "SubSGD iter. 258/499: loss=159.51611627118677, w0=72.80000000000018, w1=16.13855280713239\n",
      "SubSGD iter. 259/499: loss=162.02393159138097, w0=72.27500000000019, w1=16.226071857068934\n",
      "SubSGD iter. 260/499: loss=163.64304210993038, w0=72.18750000000017, w1=16.349594462076723\n",
      "SubSGD iter. 261/499: loss=159.13123502900874, w0=72.71250000000018, w1=16.07800113726723\n",
      "SubSGD iter. 262/499: loss=158.36770144615505, w0=72.71250000000018, w1=16.00172356049338\n",
      "SubSGD iter. 263/499: loss=156.64853878522928, w0=72.71250000000019, w1=15.825534136444078\n",
      "SubSGD iter. 264/499: loss=157.68382000732788, w0=72.62500000000018, w1=15.907334816066667\n",
      "SubSGD iter. 265/499: loss=158.72366186662543, w0=72.71250000000018, w1=16.03742899391082\n",
      "SubSGD iter. 266/499: loss=156.84271941907744, w0=72.88750000000016, w1=15.89165688431959\n",
      "SubSGD iter. 267/499: loss=155.81255756088908, w0=72.36250000000015, w1=15.622163039623246\n",
      "SubSGD iter. 268/499: loss=156.0739601274136, w0=72.62500000000016, w1=15.739234478219227\n",
      "SubSGD iter. 269/499: loss=155.22343316538596, w0=72.62500000000016, w1=15.64795071056435\n",
      "SubSGD iter. 270/499: loss=153.58758816096253, w0=72.62500000000017, w1=15.467099646992171\n",
      "SubSGD iter. 271/499: loss=154.94586528405122, w0=72.71250000000018, w1=15.644401468484348\n",
      "SubSGD iter. 272/499: loss=161.73548824462176, w0=72.27500000000018, w1=16.198215038703303\n",
      "SubSGD iter. 273/499: loss=158.43231089959298, w0=72.97500000000016, w1=16.07241668309573\n",
      "SubSGD iter. 274/499: loss=155.72875959303838, w0=72.80000000000017, w1=15.7529418880149\n",
      "SubSGD iter. 275/499: loss=160.11241246195448, w0=72.45000000000017, w1=16.097864002031137\n",
      "SubSGD iter. 276/499: loss=157.90914658032975, w0=72.53750000000018, w1=15.903760274428901\n",
      "SubSGD iter. 277/499: loss=163.46594120373274, w0=72.88750000000019, w1=16.53138093322815\n",
      "SubSGD iter. 278/499: loss=159.491764878411, w0=72.80000000000021, w1=16.136166568948983\n",
      "SubSGD iter. 279/499: loss=158.05262898683716, w0=72.80000000000021, w1=15.993103927318144\n",
      "SubSGD iter. 280/499: loss=156.650876970184, w0=72.53750000000021, w1=15.77278266070902\n",
      "SubSGD iter. 281/499: loss=155.03217708104577, w0=72.7125000000002, w1=15.653754348095251\n",
      "SubSGD iter. 282/499: loss=156.30819038332027, w0=72.62500000000021, w1=15.764064078768905\n",
      "SubSGD iter. 283/499: loss=154.68694814711952, w0=73.06250000000018, w1=15.705531661969275\n",
      "SubSGD iter. 284/499: loss=158.397548550144, w0=73.06250000000018, w1=16.087166482102827\n",
      "SubSGD iter. 285/499: loss=160.6507207449743, w0=72.6250000000002, w1=16.20283176621905\n",
      "SubSGD iter. 286/499: loss=160.844912386425, w0=72.4500000000002, w1=16.169693440128583\n",
      "SubSGD iter. 287/499: loss=159.189079849952, w0=72.36250000000021, w1=15.97650913522338\n",
      "SubSGD iter. 288/499: loss=161.10984014629565, w0=72.36250000000021, w1=16.16717621858889\n",
      "SubSGD iter. 289/499: loss=156.2440766349773, w0=72.62500000000023, w1=15.757280671359243\n",
      "SubSGD iter. 290/499: loss=156.82680805138008, w0=72.88750000000023, w1=15.890018544640391\n",
      "SubSGD iter. 291/499: loss=157.99071120942568, w0=72.88750000000026, w1=16.00843777754079\n",
      "SubSGD iter. 292/499: loss=160.2721668645456, w0=72.71250000000028, w1=16.189878153929218\n",
      "SubSGD iter. 293/499: loss=156.11632578134444, w0=72.97500000000028, w1=15.837044482523028\n",
      "SubSGD iter. 294/499: loss=161.0078455291546, w0=72.36250000000028, w1=16.157230003902537\n",
      "SubSGD iter. 295/499: loss=158.36432487315253, w0=72.36250000000027, w1=15.892342406529353\n",
      "SubSGD iter. 296/499: loss=158.61353213745238, w0=72.45000000000026, w1=15.947610626951915\n",
      "SubSGD iter. 297/499: loss=160.00678556335342, w0=72.80000000000027, w1=16.18639888640506\n",
      "SubSGD iter. 298/499: loss=159.3572425118228, w0=72.71250000000026, w1=16.100359143448678\n",
      "SubSGD iter. 299/499: loss=159.43600865793655, w0=72.97500000000026, w1=16.17107280451197\n",
      "SubSGD iter. 300/499: loss=157.61008304343815, w0=72.80000000000025, w1=15.948273657864043\n",
      "SubSGD iter. 301/499: loss=159.05216330972837, w0=72.45000000000024, w1=15.992052047306675\n",
      "SubSGD iter. 302/499: loss=158.92527621427914, w0=72.36250000000024, w1=15.949744604305327\n",
      "SubSGD iter. 303/499: loss=158.93971221697515, w0=72.71250000000023, w1=16.05897667776042\n",
      "SubSGD iter. 304/499: loss=157.50577066349885, w0=72.80000000000024, w1=15.937647029061099\n",
      "SubSGD iter. 305/499: loss=155.66236847783455, w0=72.97500000000025, w1=15.789543500332865\n",
      "SubSGD iter. 306/499: loss=158.588870697236, w0=72.62500000000027, w1=15.999340631330913\n",
      "SubSGD iter. 307/499: loss=160.70209832916774, w0=72.80000000000025, w1=16.253448922255654\n",
      "SubSGD iter. 308/499: loss=157.40972407804566, w0=72.88750000000024, w1=15.949682955181357\n",
      "SubSGD iter. 309/499: loss=157.1200973740169, w0=72.71250000000023, w1=15.87450055327532\n",
      "SubSGD iter. 310/499: loss=155.08575759049813, w0=72.80000000000024, w1=15.684299741831431\n",
      "SubSGD iter. 311/499: loss=156.43207888038324, w0=72.53750000000024, w1=15.749635862645793\n",
      "SubSGD iter. 312/499: loss=155.9982887221538, w0=72.53750000000024, w1=15.70340560361474\n",
      "SubSGD iter. 313/499: loss=153.33400176592437, w0=72.7125000000002, w1=15.466112260827733\n",
      "SubSGD iter. 314/499: loss=155.31615932781085, w0=72.6250000000002, w1=15.65799019137391\n",
      "SubSGD iter. 315/499: loss=155.6253973275016, w0=72.80000000000018, w1=15.74197521759876\n",
      "SubSGD iter. 316/499: loss=157.45946780317405, w0=72.6250000000002, w1=15.88425759349857\n",
      "SubSGD iter. 317/499: loss=161.74989464632947, w0=72.36250000000018, w1=16.229157699373374\n",
      "SubSGD iter. 318/499: loss=161.1829881991504, w0=72.5375000000002, w1=16.22909278138685\n",
      "SubSGD iter. 319/499: loss=158.9406358045602, w0=72.62500000000018, w1=16.034643546107354\n",
      "SubSGD iter. 320/499: loss=157.95667399737286, w0=73.06250000000018, w1=16.043345118334354\n",
      "SubSGD iter. 321/499: loss=160.0215504812773, w0=72.53750000000016, w1=16.116061490539916\n",
      "SubSGD iter. 322/499: loss=156.47508863047796, w0=72.97500000000016, w1=15.874254719183552\n",
      "SubSGD iter. 323/499: loss=157.75728057264755, w0=73.06250000000017, w1=16.023399702660296\n",
      "SubSGD iter. 324/499: loss=157.17131794715505, w0=72.8875000000002, w1=15.925368854343944\n",
      "SubSGD iter. 325/499: loss=154.08867766993237, w0=72.71250000000022, w1=15.550463878232025\n",
      "SubSGD iter. 326/499: loss=158.01132121562682, w0=72.45000000000019, w1=15.885932407194758\n",
      "SubSGD iter. 327/499: loss=154.53013426790446, w0=72.71250000000019, w1=15.59908374270099\n",
      "SubSGD iter. 328/499: loss=155.9132445861695, w0=72.5375000000002, w1=15.694288428604459\n",
      "SubSGD iter. 329/499: loss=160.1520408062293, w0=72.45000000000019, w1=16.10177603854379\n",
      "SubSGD iter. 330/499: loss=155.18504251319692, w0=72.5375000000002, w1=15.61547848583154\n",
      "SubSGD iter. 331/499: loss=156.1197635736276, w0=72.8000000000002, w1=15.794198563236819\n",
      "SubSGD iter. 332/499: loss=156.5867882158296, w0=73.06250000000018, w1=15.90466777123571\n",
      "SubSGD iter. 333/499: loss=155.33809686224672, w0=72.97500000000018, w1=15.755319849026012\n",
      "SubSGD iter. 334/499: loss=153.0470218701101, w0=73.06250000000018, w1=15.526545097580433\n",
      "SubSGD iter. 335/499: loss=157.46178358918675, w0=72.97500000000016, w1=15.975147769173212\n",
      "SubSGD iter. 336/499: loss=158.6775330689541, w0=72.88750000000016, w1=16.07701257113783\n",
      "SubSGD iter. 337/499: loss=158.20381581055665, w0=72.97500000000015, w1=16.049686084852446\n",
      "SubSGD iter. 338/499: loss=158.91099367698817, w0=72.62500000000016, w1=16.031678305039875\n",
      "SubSGD iter. 339/499: loss=157.001023908524, w0=72.80000000000015, w1=15.885898063012586\n",
      "SubSGD iter. 340/499: loss=159.07143389021078, w0=72.80000000000015, w1=16.094800200994772\n",
      "SubSGD iter. 341/499: loss=162.97559294798774, w0=72.71250000000013, w1=16.445743115790947\n",
      "SubSGD iter. 342/499: loss=160.72889899016522, w0=72.71250000000013, w1=16.233989481839167\n",
      "SubSGD iter. 343/499: loss=160.83294797095297, w0=72.62500000000013, w1=16.220432353148635\n",
      "SubSGD iter. 344/499: loss=158.89574318431346, w0=72.80000000000013, w1=16.07740924805302\n",
      "SubSGD iter. 345/499: loss=158.37977223161846, w0=72.97500000000011, w1=16.067199247666608\n",
      "SubSGD iter. 346/499: loss=155.03921267030722, w0=72.8000000000001, w1=15.67929157377744\n",
      "SubSGD iter. 347/499: loss=157.35454905833208, w0=72.71250000000012, w1=15.898662427533134\n",
      "SubSGD iter. 348/499: loss=155.231427368541, w0=72.53750000000011, w1=15.620538894226867\n",
      "SubSGD iter. 349/499: loss=156.12346911830053, w0=72.53750000000011, w1=15.71679329541623\n",
      "SubSGD iter. 350/499: loss=156.21063234730346, w0=72.71250000000012, w1=15.779609628933166\n",
      "SubSGD iter. 351/499: loss=157.51788769902626, w0=72.71250000000013, w1=15.915424912349046\n",
      "SubSGD iter. 352/499: loss=155.27796547723122, w0=72.62500000000016, w1=15.653857578223867\n",
      "SubSGD iter. 353/499: loss=159.6535230120595, w0=72.62500000000014, w1=16.105434686545035\n",
      "SubSGD iter. 354/499: loss=157.2335955635405, w0=72.71250000000013, w1=15.886212356932816\n",
      "SubSGD iter. 355/499: loss=160.63303236973974, w0=72.88750000000014, w1=16.26733695867965\n",
      "SubSGD iter. 356/499: loss=158.17545819569548, w0=72.62500000000014, w1=15.957527216213863\n",
      "SubSGD iter. 357/499: loss=161.01202741219927, w0=72.36250000000013, w1=16.15763818848212\n",
      "SubSGD iter. 358/499: loss=162.38746957310298, w0=72.36250000000011, w1=16.29017233811657\n",
      "SubSGD iter. 359/499: loss=161.06833727990045, w0=72.27500000000012, w1=16.133200882818212\n",
      "SubSGD iter. 360/499: loss=161.0145958332796, w0=72.62500000000011, w1=16.237917727082774\n",
      "SubSGD iter. 361/499: loss=158.06606073268952, w0=72.71250000000013, w1=15.971264642159184\n",
      "SubSGD iter. 362/499: loss=159.2668527202766, w0=72.71250000000016, w1=16.091429109388688\n",
      "SubSGD iter. 363/499: loss=157.56831282407006, w0=72.97500000000015, w1=15.985917788810434\n",
      "SubSGD iter. 364/499: loss=156.12823552992984, w0=73.06250000000014, w1=15.857356422370254\n",
      "SubSGD iter. 365/499: loss=158.1166507047319, w0=72.80000000000015, w1=15.999555862848807\n",
      "SubSGD iter. 366/499: loss=156.32318179692382, w0=72.71250000000012, w1=15.791455349292105\n",
      "SubSGD iter. 367/499: loss=158.77906754174356, w0=72.71250000000012, w1=16.042963693006104\n",
      "SubSGD iter. 368/499: loss=158.09661205929905, w0=72.7125000000001, w1=15.974358186401034\n",
      "SubSGD iter. 369/499: loss=160.36942692933778, w0=73.0625000000001, w1=16.27868422695527\n",
      "SubSGD iter. 370/499: loss=159.86816167657042, w0=72.88750000000009, w1=16.193729963197622\n",
      "SubSGD iter. 371/499: loss=160.23714764655136, w0=72.80000000000008, w1=16.208709295939144\n",
      "SubSGD iter. 372/499: loss=158.01005656497378, w0=72.8000000000001, w1=15.988808942149662\n",
      "SubSGD iter. 373/499: loss=160.76705980707808, w0=72.62500000000009, w1=16.21407538677246\n",
      "SubSGD iter. 374/499: loss=158.1225461885953, w0=72.88750000000007, w1=16.021673793534944\n",
      "SubSGD iter. 375/499: loss=159.55323978881168, w0=72.88750000000006, w1=16.163116553127015\n",
      "SubSGD iter. 376/499: loss=157.85702340003613, w0=72.88750000000006, w1=15.994979672084263\n",
      "SubSGD iter. 377/499: loss=160.7442439721666, w0=72.53750000000004, w1=16.18668559905834\n",
      "SubSGD iter. 378/499: loss=163.28071268456912, w0=72.53750000000005, w1=16.427242821169724\n",
      "SubSGD iter. 379/499: loss=161.09176592601784, w0=72.62500000000006, w1=16.245328339876036\n",
      "SubSGD iter. 380/499: loss=159.30868812988876, w0=72.97500000000004, w1=16.15866333149579\n",
      "SubSGD iter. 381/499: loss=160.01107241930148, w0=72.88750000000003, w1=16.207562485595194\n",
      "SubSGD iter. 382/499: loss=159.96670657479368, w0=73.06250000000004, w1=16.240142843524637\n",
      "SubSGD iter. 383/499: loss=158.79994786455774, w0=72.71250000000005, w1=16.045047927418807\n",
      "SubSGD iter. 384/499: loss=156.9278542933218, w0=72.80000000000004, w1=15.87835049212315\n",
      "SubSGD iter. 385/499: loss=160.99211598174062, w0=72.36250000000003, w1=16.155694385369628\n",
      "SubSGD iter. 386/499: loss=161.6277757834213, w0=72.36250000000003, w1=16.217389199740804\n",
      "SubSGD iter. 387/499: loss=163.38433877250466, w0=72.18750000000003, w1=16.32520000456359\n",
      "SubSGD iter. 388/499: loss=161.36984313074083, w0=72.18750000000001, w1=16.131249721909533\n",
      "SubSGD iter. 389/499: loss=157.7978557153148, w0=72.45, w1=15.86388023828772\n",
      "SubSGD iter. 390/499: loss=161.40167160514966, w0=72.36249999999998, w1=16.195528871719088\n",
      "SubSGD iter. 391/499: loss=162.63161881288022, w0=72.5375, w1=16.366716561999908\n",
      "SubSGD iter. 392/499: loss=162.41196696330238, w0=72.71250000000002, w1=16.393407047341125\n",
      "SubSGD iter. 393/499: loss=157.12691908477512, w0=72.45000000000003, w1=15.793903491252818\n",
      "SubSGD iter. 394/499: loss=158.6637912729643, w0=72.53750000000002, w1=15.980650122583965\n",
      "SubSGD iter. 395/499: loss=160.68283448148136, w0=72.62500000000001, w1=16.2059378498817\n",
      "SubSGD iter. 396/499: loss=157.88156634195482, w0=72.45000000000002, w1=15.872539990687077\n",
      "SubSGD iter. 397/499: loss=165.3813173437674, w0=72.53750000000002, w1=16.618623112325846\n",
      "SubSGD iter. 398/499: loss=160.84247855542426, w0=72.53750000000004, w1=16.196210816461008\n",
      "SubSGD iter. 399/499: loss=159.44929728462668, w0=72.88750000000005, w1=16.152972158122015\n",
      "SubSGD iter. 400/499: loss=157.05076338229952, w0=72.71250000000005, w1=15.86733205820332\n",
      "SubSGD iter. 401/499: loss=159.48368228280023, w0=72.53750000000008, w1=16.062855294231532\n",
      "SubSGD iter. 402/499: loss=160.2127404692404, w0=72.27500000000009, w1=16.048588856509042\n",
      "SubSGD iter. 403/499: loss=159.97277802653778, w0=72.6250000000001, w1=16.136818923275875\n",
      "SubSGD iter. 404/499: loss=160.42915746538284, w0=72.5375000000001, w1=16.15601400642652\n",
      "SubSGD iter. 405/499: loss=157.76003948501406, w0=72.8000000000001, w1=15.963510046140826\n",
      "SubSGD iter. 406/499: loss=159.07341342791196, w0=72.45000000000009, w1=15.994194961614639\n",
      "SubSGD iter. 407/499: loss=156.87746116285797, w0=72.71250000000009, w1=15.849367526338705\n",
      "SubSGD iter. 408/499: loss=161.37606945569576, w0=72.71250000000009, w1=16.295860109182748\n",
      "SubSGD iter. 409/499: loss=161.3787531979552, w0=72.71250000000009, w1=16.296115164774456\n",
      "SubSGD iter. 410/499: loss=159.6437838758925, w0=73.15000000000009, w1=16.22526634434677\n",
      "SubSGD iter. 411/499: loss=153.79637717871412, w0=72.88750000000009, w1=15.567200577471297\n",
      "SubSGD iter. 412/499: loss=157.5790235245934, w0=72.53750000000011, w1=15.869739983442717\n",
      "SubSGD iter. 413/499: loss=152.6491749327326, w0=72.71250000000012, w1=15.38815350373721\n",
      "SubSGD iter. 414/499: loss=154.8036861521522, w0=72.71250000000012, w1=15.62895320111591\n",
      "SubSGD iter. 415/499: loss=156.8230745974968, w0=72.80000000000011, w1=15.86752176101263\n",
      "SubSGD iter. 416/499: loss=161.29568690810478, w0=72.1875000000001, w1=16.123969103493213\n",
      "SubSGD iter. 417/499: loss=159.0273368006298, w0=72.36250000000008, w1=15.960116539840664\n",
      "SubSGD iter. 418/499: loss=156.97036085056035, w0=72.80000000000008, w1=15.882736546527797\n",
      "SubSGD iter. 419/499: loss=158.4551528549547, w0=72.8000000000001, w1=16.0335307763113\n",
      "SubSGD iter. 420/499: loss=159.84191160602444, w0=72.71250000000009, w1=16.147975878492254\n",
      "SubSGD iter. 421/499: loss=160.11610566113225, w0=72.7125000000001, w1=16.17471893841883\n",
      "SubSGD iter. 422/499: loss=156.92129225275005, w0=73.0625000000001, w1=15.93889243792919\n",
      "SubSGD iter. 423/499: loss=156.3239632692347, w0=72.97500000000011, w1=15.858615258742692\n",
      "SubSGD iter. 424/499: loss=160.37043757521906, w0=72.5375000000001, w1=16.15027768259146\n",
      "SubSGD iter. 425/499: loss=162.19772025764902, w0=72.53750000000011, w1=16.325870583455654\n",
      "SubSGD iter. 426/499: loss=158.7526426892292, w0=72.71250000000012, w1=16.04032476996589\n",
      "SubSGD iter. 427/499: loss=161.23432549995553, w0=72.88750000000012, w1=16.324483071471224\n",
      "SubSGD iter. 428/499: loss=159.49818484842498, w0=72.62500000000011, w1=16.090093764965165\n",
      "SubSGD iter. 429/499: loss=158.54743177553163, w0=72.80000000000011, w1=16.042752679919086\n",
      "SubSGD iter. 430/499: loss=159.64788839033494, w0=72.80000000000011, w1=16.15144609697887\n",
      "SubSGD iter. 431/499: loss=161.5569640748522, w0=72.88750000000013, w1=16.35489317035653\n",
      "SubSGD iter. 432/499: loss=158.87688243683453, w0=73.06250000000013, w1=16.13438330794863\n",
      "SubSGD iter. 433/499: loss=156.20469337036994, w0=72.62500000000014, w1=15.753108980477105\n",
      "SubSGD iter. 434/499: loss=154.9115580734092, w0=72.62500000000013, w1=15.6140224175204\n",
      "SubSGD iter. 435/499: loss=160.26124507047246, w0=72.53750000000014, w1=16.1395935696977\n",
      "SubSGD iter. 436/499: loss=157.62910567019475, w0=72.97500000000014, w1=15.992053409794913\n",
      "SubSGD iter. 437/499: loss=157.04700293533125, w0=72.45000000000014, w1=15.785499769145218\n",
      "SubSGD iter. 438/499: loss=159.04774862030473, w0=72.53750000000014, w1=16.019315835918956\n",
      "SubSGD iter. 439/499: loss=153.9639285149068, w0=72.88750000000016, w1=15.585647155284464\n",
      "SubSGD iter. 440/499: loss=156.28060562639288, w0=72.53750000000018, w1=15.733544609432933\n",
      "SubSGD iter. 441/499: loss=157.56946141734213, w0=72.53750000000018, w1=15.8687510080887\n",
      "SubSGD iter. 442/499: loss=158.35604707491095, w0=72.27500000000018, w1=15.859875076958529\n",
      "SubSGD iter. 443/499: loss=156.4593283444088, w0=72.4500000000002, w1=15.723239174481032\n",
      "SubSGD iter. 444/499: loss=156.67524919444304, w0=72.62500000000021, w1=15.802714100631613\n",
      "SubSGD iter. 445/499: loss=155.5836519708506, w0=72.71250000000022, w1=15.71307162678457\n",
      "SubSGD iter. 446/499: loss=154.06526291035303, w0=72.80000000000021, w1=15.573228372835116\n",
      "SubSGD iter. 447/499: loss=156.00717415152488, w0=72.97500000000021, w1=15.825666130593294\n",
      "SubSGD iter. 448/499: loss=155.85642853319942, w0=72.45000000000019, w1=15.658494388413084\n",
      "SubSGD iter. 449/499: loss=155.6160043910906, w0=72.62500000000018, w1=15.690306281844265\n",
      "SubSGD iter. 450/499: loss=154.6036108732957, w0=72.8000000000002, w1=15.63215698406589\n",
      "SubSGD iter. 451/499: loss=160.09569435412266, w0=72.5375000000002, w1=16.123352209752003\n",
      "SubSGD iter. 452/499: loss=156.40930352830122, w0=72.7125000000002, w1=15.800499692524488\n",
      "SubSGD iter. 453/499: loss=156.27585341525264, w0=72.53750000000021, w1=15.733038882558265\n",
      "SubSGD iter. 454/499: loss=162.07525465847482, w0=72.53750000000021, w1=16.314284834881985\n",
      "SubSGD iter. 455/499: loss=160.3993448626411, w0=72.88750000000022, w1=16.244958468888917\n",
      "SubSGD iter. 456/499: loss=159.8848777878464, w0=72.4500000000002, w1=16.075343437138923\n",
      "SubSGD iter. 457/499: loss=160.40545386313926, w0=72.45000000000019, w1=16.126721441364495\n",
      "SubSGD iter. 458/499: loss=158.31636329856426, w0=72.80000000000017, w1=16.019628701167502\n",
      "SubSGD iter. 459/499: loss=160.57512373090964, w0=72.97500000000018, w1=16.280792204780813\n",
      "SubSGD iter. 460/499: loss=155.48109647833658, w0=72.9750000000002, w1=15.770442525612951\n",
      "SubSGD iter. 461/499: loss=156.3010728897743, w0=73.06250000000017, w1=15.875243285536879\n",
      "SubSGD iter. 462/499: loss=155.3212730357797, w0=73.15000000000016, w1=15.79067505889501\n",
      "SubSGD iter. 463/499: loss=160.9634383030538, w0=72.71250000000018, w1=16.256496549136703\n",
      "SubSGD iter. 464/499: loss=158.798896515201, w0=72.88750000000019, w1=16.089033209843695\n",
      "SubSGD iter. 465/499: loss=160.8889408851235, w0=72.8000000000002, w1=16.271320077345763\n",
      "SubSGD iter. 466/499: loss=160.94720230863837, w0=72.62500000000021, w1=16.231437309616638\n",
      "SubSGD iter. 467/499: loss=161.87987790259626, w0=72.45000000000024, w1=16.26950296145874\n",
      "SubSGD iter. 468/499: loss=159.80602316903725, w0=72.53750000000024, w1=16.09480859590677\n",
      "SubSGD iter. 469/499: loss=155.94380066891003, w0=72.36250000000024, w1=15.636445952835576\n",
      "SubSGD iter. 470/499: loss=158.17198450795973, w0=72.27500000000025, w1=15.840763306686956\n",
      "SubSGD iter. 471/499: loss=155.84274052004167, w0=72.62500000000024, w1=15.714594811479818\n",
      "SubSGD iter. 472/499: loss=151.985307853698, w0=72.80000000000025, w1=15.33797701285852\n",
      "SubSGD iter. 473/499: loss=155.40120957936554, w0=72.62500000000027, w1=15.667179468552915\n",
      "SubSGD iter. 474/499: loss=158.58741758174227, w0=72.53750000000029, w1=15.972922950854002\n",
      "SubSGD iter. 475/499: loss=158.30229488379462, w0=72.71250000000029, w1=15.995134851936454\n",
      "SubSGD iter. 476/499: loss=155.6489428378378, w0=72.45000000000029, w1=15.636001998886352\n",
      "SubSGD iter. 477/499: loss=159.04244966354764, w0=72.3625000000003, w1=15.961650532702826\n",
      "SubSGD iter. 478/499: loss=160.59514334584244, w0=72.80000000000031, w1=16.24319127219055\n",
      "SubSGD iter. 479/499: loss=157.8662130973092, w0=72.97500000000029, w1=16.01591165531453\n",
      "SubSGD iter. 480/499: loss=160.64533521473598, w0=73.06250000000028, w1=16.304926663576342\n",
      "SubSGD iter. 481/499: loss=158.42304481148457, w0=72.71250000000026, w1=16.007291737305874\n",
      "SubSGD iter. 482/499: loss=161.248271136305, w0=72.45000000000027, w1=16.208821338878444\n",
      "SubSGD iter. 483/499: loss=162.18334056658313, w0=72.62500000000027, w1=16.349044998076767\n",
      "SubSGD iter. 484/499: loss=158.05037274492608, w0=72.97500000000026, w1=16.034363628083586\n",
      "SubSGD iter. 485/499: loss=155.17553222518288, w0=72.71250000000025, w1=15.66924682279807\n",
      "SubSGD iter. 486/499: loss=158.6061446782126, w0=72.62500000000024, w1=16.0010800959213\n",
      "SubSGD iter. 487/499: loss=156.28947957979076, w0=72.71250000000023, w1=15.787911320219715\n",
      "SubSGD iter. 488/499: loss=161.61486959162892, w0=72.45000000000023, w1=16.244128360678015\n",
      "SubSGD iter. 489/499: loss=156.75823838172786, w0=72.4500000000002, w1=15.755009524796423\n",
      "SubSGD iter. 490/499: loss=154.3951061093213, w0=72.45000000000022, w1=15.497668680544486\n",
      "SubSGD iter. 491/499: loss=155.88454343195824, w0=72.62500000000023, w1=15.71905909151988\n",
      "SubSGD iter. 492/499: loss=155.74822368520353, w0=72.80000000000024, w1=15.755004164957908\n",
      "SubSGD iter. 493/499: loss=160.03231400245383, w0=72.53750000000022, w1=16.117120538627603\n",
      "SubSGD iter. 494/499: loss=161.9339511149903, w0=72.80000000000022, w1=16.37016985646509\n",
      "SubSGD iter. 495/499: loss=158.44484350035003, w0=72.71250000000023, w1=16.009483224310443\n",
      "SubSGD iter. 496/499: loss=156.92866024476703, w0=72.71250000000023, w1=15.854681798952347\n",
      "SubSGD iter. 497/499: loss=157.57580593523238, w0=72.71250000000023, w1=15.921354910809747\n",
      "SubSGD iter. 498/499: loss=153.6602042777637, w0=72.97500000000024, w1=15.574115001210144\n",
      "SubSGD iter. 499/499: loss=155.1977355479213, w0=72.45000000000022, w1=15.586705446239693\n",
      "SubSGD: execution time=0.147 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 16\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6559763d5f49c9becfba577df386b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses, subsgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
